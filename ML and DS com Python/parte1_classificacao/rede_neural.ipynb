{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from yellowbrick.classifier import ConfusionMatrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit.pkl', 'rb') as f:\n",
    "    X_credit_treino, y_credit_treino, X_credit_teste, y_credit_teste = pickle.load(f)\n",
    "\n",
    "with open('census.pkl', 'rb') as f:\n",
    "    X_census_treino, y_census_treino, X_census_teste, y_census_teste = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.62448699\n",
      "Iteration 2, loss = 0.61804677\n",
      "Iteration 3, loss = 0.61174128\n",
      "Iteration 4, loss = 0.60545028\n",
      "Iteration 5, loss = 0.59944283\n",
      "Iteration 6, loss = 0.59349165\n",
      "Iteration 7, loss = 0.58775495\n",
      "Iteration 8, loss = 0.58215447\n",
      "Iteration 9, loss = 0.57657844\n",
      "Iteration 10, loss = 0.57122739\n",
      "Iteration 11, loss = 0.56590287\n",
      "Iteration 12, loss = 0.56075965\n",
      "Iteration 13, loss = 0.55579075\n",
      "Iteration 14, loss = 0.55085416\n",
      "Iteration 15, loss = 0.54612491\n",
      "Iteration 16, loss = 0.54153222\n",
      "Iteration 17, loss = 0.53698366\n",
      "Iteration 18, loss = 0.53261432\n",
      "Iteration 19, loss = 0.52829132\n",
      "Iteration 20, loss = 0.52407161\n",
      "Iteration 21, loss = 0.52001917\n",
      "Iteration 22, loss = 0.51596368\n",
      "Iteration 23, loss = 0.51209796\n",
      "Iteration 24, loss = 0.50829963\n",
      "Iteration 25, loss = 0.50457432\n",
      "Iteration 26, loss = 0.50093477\n",
      "Iteration 27, loss = 0.49747871\n",
      "Iteration 28, loss = 0.49395928\n",
      "Iteration 29, loss = 0.49054542\n",
      "Iteration 30, loss = 0.48724807\n",
      "Iteration 31, loss = 0.48404220\n",
      "Iteration 32, loss = 0.48083163\n",
      "Iteration 33, loss = 0.47774138\n",
      "Iteration 34, loss = 0.47465465\n",
      "Iteration 35, loss = 0.47151185\n",
      "Iteration 36, loss = 0.46845183\n",
      "Iteration 37, loss = 0.46540161\n",
      "Iteration 38, loss = 0.46238319\n",
      "Iteration 39, loss = 0.45935647\n",
      "Iteration 40, loss = 0.45629856\n",
      "Iteration 41, loss = 0.45320136\n",
      "Iteration 42, loss = 0.45009158\n",
      "Iteration 43, loss = 0.44690825\n",
      "Iteration 44, loss = 0.44369528\n",
      "Iteration 45, loss = 0.44038834\n",
      "Iteration 46, loss = 0.43700935\n",
      "Iteration 47, loss = 0.43355408\n",
      "Iteration 48, loss = 0.43007716\n",
      "Iteration 49, loss = 0.42644578\n",
      "Iteration 50, loss = 0.42275839\n",
      "Iteration 51, loss = 0.41900938\n",
      "Iteration 52, loss = 0.41508850\n",
      "Iteration 53, loss = 0.41106859\n",
      "Iteration 54, loss = 0.40687837\n",
      "Iteration 55, loss = 0.40259988\n",
      "Iteration 56, loss = 0.39825268\n",
      "Iteration 57, loss = 0.39394968\n",
      "Iteration 58, loss = 0.38949741\n",
      "Iteration 59, loss = 0.38515554\n",
      "Iteration 60, loss = 0.38082043\n",
      "Iteration 61, loss = 0.37651413\n",
      "Iteration 62, loss = 0.37225349\n",
      "Iteration 63, loss = 0.36801041\n",
      "Iteration 64, loss = 0.36382006\n",
      "Iteration 65, loss = 0.35960911\n",
      "Iteration 66, loss = 0.35545939\n",
      "Iteration 67, loss = 0.35135889\n",
      "Iteration 68, loss = 0.34723392\n",
      "Iteration 69, loss = 0.34315776\n",
      "Iteration 70, loss = 0.33918151\n",
      "Iteration 71, loss = 0.33521353\n",
      "Iteration 72, loss = 0.33133077\n",
      "Iteration 73, loss = 0.32753735\n",
      "Iteration 74, loss = 0.32379190\n",
      "Iteration 75, loss = 0.32012565\n",
      "Iteration 76, loss = 0.31653896\n",
      "Iteration 77, loss = 0.31302047\n",
      "Iteration 78, loss = 0.30963467\n",
      "Iteration 79, loss = 0.30627359\n",
      "Iteration 80, loss = 0.30299195\n",
      "Iteration 81, loss = 0.29983107\n",
      "Iteration 82, loss = 0.29656631\n",
      "Iteration 83, loss = 0.29339559\n",
      "Iteration 84, loss = 0.29034453\n",
      "Iteration 85, loss = 0.28729106\n",
      "Iteration 86, loss = 0.28437649\n",
      "Iteration 87, loss = 0.28161649\n",
      "Iteration 88, loss = 0.27885603\n",
      "Iteration 89, loss = 0.27621414\n",
      "Iteration 90, loss = 0.27361981\n",
      "Iteration 91, loss = 0.27106008\n",
      "Iteration 92, loss = 0.26859869\n",
      "Iteration 93, loss = 0.26619879\n",
      "Iteration 94, loss = 0.26385997\n",
      "Iteration 95, loss = 0.26162160\n",
      "Iteration 96, loss = 0.25944260\n",
      "Iteration 97, loss = 0.25730190\n",
      "Iteration 98, loss = 0.25524390\n",
      "Iteration 99, loss = 0.25318361\n",
      "Iteration 100, loss = 0.25129127\n",
      "Iteration 101, loss = 0.24941917\n",
      "Iteration 102, loss = 0.24759759\n",
      "Iteration 103, loss = 0.24583343\n",
      "Iteration 104, loss = 0.24414568\n",
      "Iteration 105, loss = 0.24256813\n",
      "Iteration 106, loss = 0.24090960\n",
      "Iteration 107, loss = 0.23933965\n",
      "Iteration 108, loss = 0.23779907\n",
      "Iteration 109, loss = 0.23633146\n",
      "Iteration 110, loss = 0.23486460\n",
      "Iteration 111, loss = 0.23350096\n",
      "Iteration 112, loss = 0.23209604\n",
      "Iteration 113, loss = 0.23077720\n",
      "Iteration 114, loss = 0.22946182\n",
      "Iteration 115, loss = 0.22821394\n",
      "Iteration 116, loss = 0.22695184\n",
      "Iteration 117, loss = 0.22573513\n",
      "Iteration 118, loss = 0.22455186\n",
      "Iteration 119, loss = 0.22338932\n",
      "Iteration 120, loss = 0.22222106\n",
      "Iteration 121, loss = 0.22108461\n",
      "Iteration 122, loss = 0.21999463\n",
      "Iteration 123, loss = 0.21887116\n",
      "Iteration 124, loss = 0.21782387\n",
      "Iteration 125, loss = 0.21675710\n",
      "Iteration 126, loss = 0.21571733\n",
      "Iteration 127, loss = 0.21470018\n",
      "Iteration 128, loss = 0.21369743\n",
      "Iteration 129, loss = 0.21269036\n",
      "Iteration 130, loss = 0.21171180\n",
      "Iteration 131, loss = 0.21077131\n",
      "Iteration 132, loss = 0.20981635\n",
      "Iteration 133, loss = 0.20889453\n",
      "Iteration 134, loss = 0.20796905\n",
      "Iteration 135, loss = 0.20709664\n",
      "Iteration 136, loss = 0.20618318\n",
      "Iteration 137, loss = 0.20531388\n",
      "Iteration 138, loss = 0.20444250\n",
      "Iteration 139, loss = 0.20361369\n",
      "Iteration 140, loss = 0.20276801\n",
      "Iteration 141, loss = 0.20196347\n",
      "Iteration 142, loss = 0.20118183\n",
      "Iteration 143, loss = 0.20041602\n",
      "Iteration 144, loss = 0.19960078\n",
      "Iteration 145, loss = 0.19886096\n",
      "Iteration 146, loss = 0.19812960\n",
      "Iteration 147, loss = 0.19737617\n",
      "Iteration 148, loss = 0.19664026\n",
      "Iteration 149, loss = 0.19591725\n",
      "Iteration 150, loss = 0.19520157\n",
      "Iteration 151, loss = 0.19449986\n",
      "Iteration 152, loss = 0.19381474\n",
      "Iteration 153, loss = 0.19315230\n",
      "Iteration 154, loss = 0.19245949\n",
      "Iteration 155, loss = 0.19182256\n",
      "Iteration 156, loss = 0.19116534\n",
      "Iteration 157, loss = 0.19051617\n",
      "Iteration 158, loss = 0.18990843\n",
      "Iteration 159, loss = 0.18926004\n",
      "Iteration 160, loss = 0.18863518\n",
      "Iteration 161, loss = 0.18798223\n",
      "Iteration 162, loss = 0.18736368\n",
      "Iteration 163, loss = 0.18676068\n",
      "Iteration 164, loss = 0.18615638\n",
      "Iteration 165, loss = 0.18554536\n",
      "Iteration 166, loss = 0.18492847\n",
      "Iteration 167, loss = 0.18433441\n",
      "Iteration 168, loss = 0.18375193\n",
      "Iteration 169, loss = 0.18316181\n",
      "Iteration 170, loss = 0.18258327\n",
      "Iteration 171, loss = 0.18199160\n",
      "Iteration 172, loss = 0.18144255\n",
      "Iteration 173, loss = 0.18087857\n",
      "Iteration 174, loss = 0.18031418\n",
      "Iteration 175, loss = 0.17976033\n",
      "Iteration 176, loss = 0.17921241\n",
      "Iteration 177, loss = 0.17869066\n",
      "Iteration 178, loss = 0.17814995\n",
      "Iteration 179, loss = 0.17763184\n",
      "Iteration 180, loss = 0.17712675\n",
      "Iteration 181, loss = 0.17659325\n",
      "Iteration 182, loss = 0.17611286\n",
      "Iteration 183, loss = 0.17559764\n",
      "Iteration 184, loss = 0.17510681\n",
      "Iteration 185, loss = 0.17462714\n",
      "Iteration 186, loss = 0.17414648\n",
      "Iteration 187, loss = 0.17367177\n",
      "Iteration 188, loss = 0.17320411\n",
      "Iteration 189, loss = 0.17272907\n",
      "Iteration 190, loss = 0.17226862\n",
      "Iteration 191, loss = 0.17180497\n",
      "Iteration 192, loss = 0.17134369\n",
      "Iteration 193, loss = 0.17089682\n",
      "Iteration 194, loss = 0.17044954\n",
      "Iteration 195, loss = 0.16997500\n",
      "Iteration 196, loss = 0.16951310\n",
      "Iteration 197, loss = 0.16905895\n",
      "Iteration 198, loss = 0.16860998\n",
      "Iteration 199, loss = 0.16816239\n",
      "Iteration 200, loss = 0.16769163\n",
      "Iteration 201, loss = 0.16726244\n",
      "Iteration 202, loss = 0.16681165\n",
      "Iteration 203, loss = 0.16636255\n",
      "Iteration 204, loss = 0.16594163\n",
      "Iteration 205, loss = 0.16550534\n",
      "Iteration 206, loss = 0.16513624\n",
      "Iteration 207, loss = 0.16470159\n",
      "Iteration 208, loss = 0.16430246\n",
      "Iteration 209, loss = 0.16393801\n",
      "Iteration 210, loss = 0.16352969\n",
      "Iteration 211, loss = 0.16319178\n",
      "Iteration 212, loss = 0.16281294\n",
      "Iteration 213, loss = 0.16244677\n",
      "Iteration 214, loss = 0.16206511\n",
      "Iteration 215, loss = 0.16171692\n",
      "Iteration 216, loss = 0.16136332\n",
      "Iteration 217, loss = 0.16102373\n",
      "Iteration 218, loss = 0.16066533\n",
      "Iteration 219, loss = 0.16031657\n",
      "Iteration 220, loss = 0.15996704\n",
      "Iteration 221, loss = 0.15962281\n",
      "Iteration 222, loss = 0.15931693\n",
      "Iteration 223, loss = 0.15896346\n",
      "Iteration 224, loss = 0.15861617\n",
      "Iteration 225, loss = 0.15830750\n",
      "Iteration 226, loss = 0.15802401\n",
      "Iteration 227, loss = 0.15763366\n",
      "Iteration 228, loss = 0.15731797\n",
      "Iteration 229, loss = 0.15700003\n",
      "Iteration 230, loss = 0.15670453\n",
      "Iteration 231, loss = 0.15637388\n",
      "Iteration 232, loss = 0.15612671\n",
      "Iteration 233, loss = 0.15577992\n",
      "Iteration 234, loss = 0.15548951\n",
      "Iteration 235, loss = 0.15518342\n",
      "Iteration 236, loss = 0.15486077\n",
      "Iteration 237, loss = 0.15456093\n",
      "Iteration 238, loss = 0.15427475\n",
      "Iteration 239, loss = 0.15396021\n",
      "Iteration 240, loss = 0.15366937\n",
      "Iteration 241, loss = 0.15336765\n",
      "Iteration 242, loss = 0.15307589\n",
      "Iteration 243, loss = 0.15278620\n",
      "Iteration 244, loss = 0.15245995\n",
      "Iteration 245, loss = 0.15217217\n",
      "Iteration 246, loss = 0.15188262\n",
      "Iteration 247, loss = 0.15165507\n",
      "Iteration 248, loss = 0.15128616\n",
      "Iteration 249, loss = 0.15106227\n",
      "Iteration 250, loss = 0.15075154\n",
      "Iteration 251, loss = 0.15045092\n",
      "Iteration 252, loss = 0.15018190\n",
      "Iteration 253, loss = 0.14987688\n",
      "Iteration 254, loss = 0.14961854\n",
      "Iteration 255, loss = 0.14928308\n",
      "Iteration 256, loss = 0.14901935\n",
      "Iteration 257, loss = 0.14876944\n",
      "Iteration 258, loss = 0.14844728\n",
      "Iteration 259, loss = 0.14819047\n",
      "Iteration 260, loss = 0.14791617\n",
      "Iteration 261, loss = 0.14763606\n",
      "Iteration 262, loss = 0.14737236\n",
      "Iteration 263, loss = 0.14707401\n",
      "Iteration 264, loss = 0.14680353\n",
      "Iteration 265, loss = 0.14651352\n",
      "Iteration 266, loss = 0.14627353\n",
      "Iteration 267, loss = 0.14599328\n",
      "Iteration 268, loss = 0.14574169\n",
      "Iteration 269, loss = 0.14549418\n",
      "Iteration 270, loss = 0.14523744\n",
      "Iteration 271, loss = 0.14493723\n",
      "Iteration 272, loss = 0.14467475\n",
      "Iteration 273, loss = 0.14441665\n",
      "Iteration 274, loss = 0.14415474\n",
      "Iteration 275, loss = 0.14390778\n",
      "Iteration 276, loss = 0.14363740\n",
      "Iteration 277, loss = 0.14338889\n",
      "Iteration 278, loss = 0.14315878\n",
      "Iteration 279, loss = 0.14291417\n",
      "Iteration 280, loss = 0.14265905\n",
      "Iteration 281, loss = 0.14239148\n",
      "Iteration 282, loss = 0.14213847\n",
      "Iteration 283, loss = 0.14191665\n",
      "Iteration 284, loss = 0.14162104\n",
      "Iteration 285, loss = 0.14141266\n",
      "Iteration 286, loss = 0.14119825\n",
      "Iteration 287, loss = 0.14092868\n",
      "Iteration 288, loss = 0.14069837\n",
      "Iteration 289, loss = 0.14041908\n",
      "Iteration 290, loss = 0.14017592\n",
      "Iteration 291, loss = 0.13995701\n",
      "Iteration 292, loss = 0.13970092\n",
      "Iteration 293, loss = 0.13945657\n",
      "Iteration 294, loss = 0.13923090\n",
      "Iteration 295, loss = 0.13899396\n",
      "Iteration 296, loss = 0.13873318\n",
      "Iteration 297, loss = 0.13852532\n",
      "Iteration 298, loss = 0.13826782\n",
      "Iteration 299, loss = 0.13806177\n",
      "Iteration 300, loss = 0.13782789\n",
      "Iteration 301, loss = 0.13756424\n",
      "Iteration 302, loss = 0.13734435\n",
      "Iteration 303, loss = 0.13710799\n",
      "Iteration 304, loss = 0.13687752\n",
      "Iteration 305, loss = 0.13671490\n",
      "Iteration 306, loss = 0.13642400\n",
      "Iteration 307, loss = 0.13619623\n",
      "Iteration 308, loss = 0.13599977\n",
      "Iteration 309, loss = 0.13584127\n",
      "Iteration 310, loss = 0.13552551\n",
      "Iteration 311, loss = 0.13525209\n",
      "Iteration 312, loss = 0.13504009\n",
      "Iteration 313, loss = 0.13491313\n",
      "Iteration 314, loss = 0.13455804\n",
      "Iteration 315, loss = 0.13430128\n",
      "Iteration 316, loss = 0.13406357\n",
      "Iteration 317, loss = 0.13383000\n",
      "Iteration 318, loss = 0.13357825\n",
      "Iteration 319, loss = 0.13334512\n",
      "Iteration 320, loss = 0.13307286\n",
      "Iteration 321, loss = 0.13286409\n",
      "Iteration 322, loss = 0.13257834\n",
      "Iteration 323, loss = 0.13230056\n",
      "Iteration 324, loss = 0.13203952\n",
      "Iteration 325, loss = 0.13177801\n",
      "Iteration 326, loss = 0.13151747\n",
      "Iteration 327, loss = 0.13125422\n",
      "Iteration 328, loss = 0.13098886\n",
      "Iteration 329, loss = 0.13069777\n",
      "Iteration 330, loss = 0.13045812\n",
      "Iteration 331, loss = 0.13020972\n",
      "Iteration 332, loss = 0.12994213\n",
      "Iteration 333, loss = 0.12964171\n",
      "Iteration 334, loss = 0.12939716\n",
      "Iteration 335, loss = 0.12913686\n",
      "Iteration 336, loss = 0.12885941\n",
      "Iteration 337, loss = 0.12863098\n",
      "Iteration 338, loss = 0.12833225\n",
      "Iteration 339, loss = 0.12805729\n",
      "Iteration 340, loss = 0.12779778\n",
      "Iteration 341, loss = 0.12757344\n",
      "Iteration 342, loss = 0.12725968\n",
      "Iteration 343, loss = 0.12699563\n",
      "Iteration 344, loss = 0.12674528\n",
      "Iteration 345, loss = 0.12647564\n",
      "Iteration 346, loss = 0.12618234\n",
      "Iteration 347, loss = 0.12587423\n",
      "Iteration 348, loss = 0.12554180\n",
      "Iteration 349, loss = 0.12523029\n",
      "Iteration 350, loss = 0.12492923\n",
      "Iteration 351, loss = 0.12456101\n",
      "Iteration 352, loss = 0.12420156\n",
      "Iteration 353, loss = 0.12386472\n",
      "Iteration 354, loss = 0.12352740\n",
      "Iteration 355, loss = 0.12319239\n",
      "Iteration 356, loss = 0.12283674\n",
      "Iteration 357, loss = 0.12248299\n",
      "Iteration 358, loss = 0.12216384\n",
      "Iteration 359, loss = 0.12179095\n",
      "Iteration 360, loss = 0.12146755\n",
      "Iteration 361, loss = 0.12108941\n",
      "Iteration 362, loss = 0.12075007\n",
      "Iteration 363, loss = 0.12040248\n",
      "Iteration 364, loss = 0.12007128\n",
      "Iteration 365, loss = 0.11970009\n",
      "Iteration 366, loss = 0.11936386\n",
      "Iteration 367, loss = 0.11899780\n",
      "Iteration 368, loss = 0.11866974\n",
      "Iteration 369, loss = 0.11835482\n",
      "Iteration 370, loss = 0.11795205\n",
      "Iteration 371, loss = 0.11764731\n",
      "Iteration 372, loss = 0.11731498\n",
      "Iteration 373, loss = 0.11695475\n",
      "Iteration 374, loss = 0.11658916\n",
      "Iteration 375, loss = 0.11622741\n",
      "Iteration 376, loss = 0.11593043\n",
      "Iteration 377, loss = 0.11563050\n",
      "Iteration 378, loss = 0.11524474\n",
      "Iteration 379, loss = 0.11494215\n",
      "Iteration 380, loss = 0.11457215\n",
      "Iteration 381, loss = 0.11424674\n",
      "Iteration 382, loss = 0.11391680\n",
      "Iteration 383, loss = 0.11357878\n",
      "Iteration 384, loss = 0.11326514\n",
      "Iteration 385, loss = 0.11290579\n",
      "Iteration 386, loss = 0.11260141\n",
      "Iteration 387, loss = 0.11228066\n",
      "Iteration 388, loss = 0.11193684\n",
      "Iteration 389, loss = 0.11162866\n",
      "Iteration 390, loss = 0.11130469\n",
      "Iteration 391, loss = 0.11096662\n",
      "Iteration 392, loss = 0.11065239\n",
      "Iteration 393, loss = 0.11038682\n",
      "Iteration 394, loss = 0.11009731\n",
      "Iteration 395, loss = 0.10969436\n",
      "Iteration 396, loss = 0.10937824\n",
      "Iteration 397, loss = 0.10901687\n",
      "Iteration 398, loss = 0.10869019\n",
      "Iteration 399, loss = 0.10833777\n",
      "Iteration 400, loss = 0.10797768\n",
      "Iteration 401, loss = 0.10761620\n",
      "Iteration 402, loss = 0.10726760\n",
      "Iteration 403, loss = 0.10691117\n",
      "Iteration 404, loss = 0.10656245\n",
      "Iteration 405, loss = 0.10617280\n",
      "Iteration 406, loss = 0.10584671\n",
      "Iteration 407, loss = 0.10545825\n",
      "Iteration 408, loss = 0.10506217\n",
      "Iteration 409, loss = 0.10473157\n",
      "Iteration 410, loss = 0.10437195\n",
      "Iteration 411, loss = 0.10396006\n",
      "Iteration 412, loss = 0.10361593\n",
      "Iteration 413, loss = 0.10320249\n",
      "Iteration 414, loss = 0.10284414\n",
      "Iteration 415, loss = 0.10250608\n",
      "Iteration 416, loss = 0.10213507\n",
      "Iteration 417, loss = 0.10173409\n",
      "Iteration 418, loss = 0.10135708\n",
      "Iteration 419, loss = 0.10101428\n",
      "Iteration 420, loss = 0.10062456\n",
      "Iteration 421, loss = 0.10024158\n",
      "Iteration 422, loss = 0.09996890\n",
      "Iteration 423, loss = 0.09951777\n",
      "Iteration 424, loss = 0.09914720\n",
      "Iteration 425, loss = 0.09881578\n",
      "Iteration 426, loss = 0.09852214\n",
      "Iteration 427, loss = 0.09808798\n",
      "Iteration 428, loss = 0.09771304\n",
      "Iteration 429, loss = 0.09733880\n",
      "Iteration 430, loss = 0.09697292\n",
      "Iteration 431, loss = 0.09660632\n",
      "Iteration 432, loss = 0.09622838\n",
      "Iteration 433, loss = 0.09583297\n",
      "Iteration 434, loss = 0.09544554\n",
      "Iteration 435, loss = 0.09504272\n",
      "Iteration 436, loss = 0.09466431\n",
      "Iteration 437, loss = 0.09427842\n",
      "Iteration 438, loss = 0.09394504\n",
      "Iteration 439, loss = 0.09350893\n",
      "Iteration 440, loss = 0.09312604\n",
      "Iteration 441, loss = 0.09272172\n",
      "Iteration 442, loss = 0.09233830\n",
      "Iteration 443, loss = 0.09196244\n",
      "Iteration 444, loss = 0.09162241\n",
      "Iteration 445, loss = 0.09129752\n",
      "Iteration 446, loss = 0.09083507\n",
      "Iteration 447, loss = 0.09049888\n",
      "Iteration 448, loss = 0.09012487\n",
      "Iteration 449, loss = 0.08979283\n",
      "Iteration 450, loss = 0.08937772\n",
      "Iteration 451, loss = 0.08901616\n",
      "Iteration 452, loss = 0.08864029\n",
      "Iteration 453, loss = 0.08830357\n",
      "Iteration 454, loss = 0.08792361\n",
      "Iteration 455, loss = 0.08751110\n",
      "Iteration 456, loss = 0.08713391\n",
      "Iteration 457, loss = 0.08679318\n",
      "Iteration 458, loss = 0.08640534\n",
      "Iteration 459, loss = 0.08605676\n",
      "Iteration 460, loss = 0.08566808\n",
      "Iteration 461, loss = 0.08530425\n",
      "Iteration 462, loss = 0.08492490\n",
      "Iteration 463, loss = 0.08457142\n",
      "Iteration 464, loss = 0.08418710\n",
      "Iteration 465, loss = 0.08381866\n",
      "Iteration 466, loss = 0.08350671\n",
      "Iteration 467, loss = 0.08315455\n",
      "Iteration 468, loss = 0.08276928\n",
      "Iteration 469, loss = 0.08243351\n",
      "Iteration 470, loss = 0.08208327\n",
      "Iteration 471, loss = 0.08175776\n",
      "Iteration 472, loss = 0.08133864\n",
      "Iteration 473, loss = 0.08098442\n",
      "Iteration 474, loss = 0.08063091\n",
      "Iteration 475, loss = 0.08027599\n",
      "Iteration 476, loss = 0.07990311\n",
      "Iteration 477, loss = 0.07954368\n",
      "Iteration 478, loss = 0.07924273\n",
      "Iteration 479, loss = 0.07879751\n",
      "Iteration 480, loss = 0.07850409\n",
      "Iteration 481, loss = 0.07814848\n",
      "Iteration 482, loss = 0.07772215\n",
      "Iteration 483, loss = 0.07743205\n",
      "Iteration 484, loss = 0.07701563\n",
      "Iteration 485, loss = 0.07670835\n",
      "Iteration 486, loss = 0.07630712\n",
      "Iteration 487, loss = 0.07598319\n",
      "Iteration 488, loss = 0.07565655\n",
      "Iteration 489, loss = 0.07533235\n",
      "Iteration 490, loss = 0.07504360\n",
      "Iteration 491, loss = 0.07465618\n",
      "Iteration 492, loss = 0.07432843\n",
      "Iteration 493, loss = 0.07402648\n",
      "Iteration 494, loss = 0.07370887\n",
      "Iteration 495, loss = 0.07337459\n",
      "Iteration 496, loss = 0.07304876\n",
      "Iteration 497, loss = 0.07273517\n",
      "Iteration 498, loss = 0.07240570\n",
      "Iteration 499, loss = 0.07212691\n",
      "Iteration 500, loss = 0.07176602\n",
      "Iteration 501, loss = 0.07146370\n",
      "Iteration 502, loss = 0.07113629\n",
      "Iteration 503, loss = 0.07085388\n",
      "Iteration 504, loss = 0.07056641\n",
      "Iteration 505, loss = 0.07027699\n",
      "Iteration 506, loss = 0.06994553\n",
      "Iteration 507, loss = 0.06964825\n",
      "Iteration 508, loss = 0.06940219\n",
      "Iteration 509, loss = 0.06908515\n",
      "Iteration 510, loss = 0.06879400\n",
      "Iteration 511, loss = 0.06851915\n",
      "Iteration 512, loss = 0.06821413\n",
      "Iteration 513, loss = 0.06794496\n",
      "Iteration 514, loss = 0.06772181\n",
      "Iteration 515, loss = 0.06740129\n",
      "Iteration 516, loss = 0.06708229\n",
      "Iteration 517, loss = 0.06678397\n",
      "Iteration 518, loss = 0.06652445\n",
      "Iteration 519, loss = 0.06622784\n",
      "Iteration 520, loss = 0.06594765\n",
      "Iteration 521, loss = 0.06566744\n",
      "Iteration 522, loss = 0.06537091\n",
      "Iteration 523, loss = 0.06510060\n",
      "Iteration 524, loss = 0.06483391\n",
      "Iteration 525, loss = 0.06452276\n",
      "Iteration 526, loss = 0.06431082\n",
      "Iteration 527, loss = 0.06404266\n",
      "Iteration 528, loss = 0.06372419\n",
      "Iteration 529, loss = 0.06348622\n",
      "Iteration 530, loss = 0.06318441\n",
      "Iteration 531, loss = 0.06293362\n",
      "Iteration 532, loss = 0.06267328\n",
      "Iteration 533, loss = 0.06244931\n",
      "Iteration 534, loss = 0.06218062\n",
      "Iteration 535, loss = 0.06190972\n",
      "Iteration 536, loss = 0.06164050\n",
      "Iteration 537, loss = 0.06139762\n",
      "Iteration 538, loss = 0.06116300\n",
      "Iteration 539, loss = 0.06092771\n",
      "Iteration 540, loss = 0.06066296\n",
      "Iteration 541, loss = 0.06049217\n",
      "Iteration 542, loss = 0.06020704\n",
      "Iteration 543, loss = 0.05994625\n",
      "Iteration 544, loss = 0.05966814\n",
      "Iteration 545, loss = 0.05945125\n",
      "Iteration 546, loss = 0.05920932\n",
      "Iteration 547, loss = 0.05896467\n",
      "Iteration 548, loss = 0.05872818\n",
      "Iteration 549, loss = 0.05851152\n",
      "Iteration 550, loss = 0.05826811\n",
      "Iteration 551, loss = 0.05804878\n",
      "Iteration 552, loss = 0.05782660\n",
      "Iteration 553, loss = 0.05762563\n",
      "Iteration 554, loss = 0.05744281\n",
      "Iteration 555, loss = 0.05721115\n",
      "Iteration 556, loss = 0.05693073\n",
      "Iteration 557, loss = 0.05674251\n",
      "Iteration 558, loss = 0.05652750\n",
      "Iteration 559, loss = 0.05633732\n",
      "Iteration 560, loss = 0.05607456\n",
      "Iteration 561, loss = 0.05585132\n",
      "Iteration 562, loss = 0.05567381\n",
      "Iteration 563, loss = 0.05545766\n",
      "Iteration 564, loss = 0.05525825\n",
      "Iteration 565, loss = 0.05502834\n",
      "Iteration 566, loss = 0.05482180\n",
      "Iteration 567, loss = 0.05462938\n",
      "Iteration 568, loss = 0.05443721\n",
      "Iteration 569, loss = 0.05422612\n",
      "Iteration 570, loss = 0.05402810\n",
      "Iteration 571, loss = 0.05384883\n",
      "Iteration 572, loss = 0.05364320\n",
      "Iteration 573, loss = 0.05344216\n",
      "Iteration 574, loss = 0.05323217\n",
      "Iteration 575, loss = 0.05305182\n",
      "Iteration 576, loss = 0.05284921\n",
      "Iteration 577, loss = 0.05267064\n",
      "Iteration 578, loss = 0.05246865\n",
      "Iteration 579, loss = 0.05234629\n",
      "Iteration 580, loss = 0.05211237\n",
      "Iteration 581, loss = 0.05194932\n",
      "Iteration 582, loss = 0.05176076\n",
      "Iteration 583, loss = 0.05158033\n",
      "Iteration 584, loss = 0.05136644\n",
      "Iteration 585, loss = 0.05119205\n",
      "Iteration 586, loss = 0.05100901\n",
      "Iteration 587, loss = 0.05082248\n",
      "Iteration 588, loss = 0.05067600\n",
      "Iteration 589, loss = 0.05045792\n",
      "Iteration 590, loss = 0.05030121\n",
      "Iteration 591, loss = 0.05011711\n",
      "Iteration 592, loss = 0.04992896\n",
      "Iteration 593, loss = 0.04981850\n",
      "Iteration 594, loss = 0.04958495\n",
      "Iteration 595, loss = 0.04947584\n",
      "Iteration 596, loss = 0.04927080\n",
      "Iteration 597, loss = 0.04904210\n",
      "Iteration 598, loss = 0.04891287\n",
      "Iteration 599, loss = 0.04872359\n",
      "Iteration 600, loss = 0.04854435\n",
      "Iteration 601, loss = 0.04838985\n",
      "Iteration 602, loss = 0.04821568\n",
      "Iteration 603, loss = 0.04804861\n",
      "Iteration 604, loss = 0.04788921\n",
      "Iteration 605, loss = 0.04772897\n",
      "Iteration 606, loss = 0.04757207\n",
      "Iteration 607, loss = 0.04739595\n",
      "Iteration 608, loss = 0.04723125\n",
      "Iteration 609, loss = 0.04708045\n",
      "Iteration 610, loss = 0.04693007\n",
      "Iteration 611, loss = 0.04676155\n",
      "Iteration 612, loss = 0.04659941\n",
      "Iteration 613, loss = 0.04642446\n",
      "Iteration 614, loss = 0.04631931\n",
      "Iteration 615, loss = 0.04615916\n",
      "Iteration 616, loss = 0.04596519\n",
      "Iteration 617, loss = 0.04582435\n",
      "Iteration 618, loss = 0.04567909\n",
      "Iteration 619, loss = 0.04548995\n",
      "Iteration 620, loss = 0.04535030\n",
      "Iteration 621, loss = 0.04521364\n",
      "Iteration 622, loss = 0.04504267\n",
      "Iteration 623, loss = 0.04490010\n",
      "Iteration 624, loss = 0.04476378\n",
      "Iteration 625, loss = 0.04463946\n",
      "Iteration 626, loss = 0.04446197\n",
      "Iteration 627, loss = 0.04434431\n",
      "Iteration 628, loss = 0.04418888\n",
      "Iteration 629, loss = 0.04402749\n",
      "Iteration 630, loss = 0.04386482\n",
      "Iteration 631, loss = 0.04373784\n",
      "Iteration 632, loss = 0.04352300\n",
      "Iteration 633, loss = 0.04338723\n",
      "Iteration 634, loss = 0.04326608\n",
      "Iteration 635, loss = 0.04306923\n",
      "Iteration 636, loss = 0.04295824\n",
      "Iteration 637, loss = 0.04276877\n",
      "Iteration 638, loss = 0.04257741\n",
      "Iteration 639, loss = 0.04240517\n",
      "Iteration 640, loss = 0.04224890\n",
      "Iteration 641, loss = 0.04206760\n",
      "Iteration 642, loss = 0.04197011\n",
      "Iteration 643, loss = 0.04176874\n",
      "Iteration 644, loss = 0.04161342\n",
      "Iteration 645, loss = 0.04150242\n",
      "Iteration 646, loss = 0.04139061\n",
      "Iteration 647, loss = 0.04122148\n",
      "Iteration 648, loss = 0.04101893\n",
      "Iteration 649, loss = 0.04085725\n",
      "Iteration 650, loss = 0.04072145\n",
      "Iteration 651, loss = 0.04058539\n",
      "Iteration 652, loss = 0.04044941\n",
      "Iteration 653, loss = 0.04032523\n",
      "Iteration 654, loss = 0.04021457\n",
      "Iteration 655, loss = 0.03996076\n",
      "Iteration 656, loss = 0.03983329\n",
      "Iteration 657, loss = 0.03970210\n",
      "Iteration 658, loss = 0.03958721\n",
      "Iteration 659, loss = 0.03943633\n",
      "Iteration 660, loss = 0.03931754\n",
      "Iteration 661, loss = 0.03915911\n",
      "Iteration 662, loss = 0.03902655\n",
      "Iteration 663, loss = 0.03887020\n",
      "Iteration 664, loss = 0.03873526\n",
      "Iteration 665, loss = 0.03859918\n",
      "Iteration 666, loss = 0.03848408\n",
      "Iteration 667, loss = 0.03831832\n",
      "Iteration 668, loss = 0.03815116\n",
      "Iteration 669, loss = 0.03803434\n",
      "Iteration 670, loss = 0.03787564\n",
      "Iteration 671, loss = 0.03772176\n",
      "Iteration 672, loss = 0.03757767\n",
      "Iteration 673, loss = 0.03747347\n",
      "Iteration 674, loss = 0.03731568\n",
      "Iteration 675, loss = 0.03719391\n",
      "Iteration 676, loss = 0.03708278\n",
      "Iteration 677, loss = 0.03689970\n",
      "Iteration 678, loss = 0.03678880\n",
      "Iteration 679, loss = 0.03664636\n",
      "Iteration 680, loss = 0.03652917\n",
      "Iteration 681, loss = 0.03639718\n",
      "Iteration 682, loss = 0.03626628\n",
      "Iteration 683, loss = 0.03620576\n",
      "Iteration 684, loss = 0.03601107\n",
      "Iteration 685, loss = 0.03588967\n",
      "Iteration 686, loss = 0.03575665\n",
      "Iteration 687, loss = 0.03564805\n",
      "Iteration 688, loss = 0.03553845\n",
      "Iteration 689, loss = 0.03536488\n",
      "Iteration 690, loss = 0.03527505\n",
      "Iteration 691, loss = 0.03512105\n",
      "Iteration 692, loss = 0.03499383\n",
      "Iteration 693, loss = 0.03486350\n",
      "Iteration 694, loss = 0.03471869\n",
      "Iteration 695, loss = 0.03456302\n",
      "Iteration 696, loss = 0.03443642\n",
      "Iteration 697, loss = 0.03433364\n",
      "Iteration 698, loss = 0.03427743\n",
      "Iteration 699, loss = 0.03408102\n",
      "Iteration 700, loss = 0.03392329\n",
      "Iteration 701, loss = 0.03377889\n",
      "Iteration 702, loss = 0.03364489\n",
      "Iteration 703, loss = 0.03354333\n",
      "Iteration 704, loss = 0.03340564\n",
      "Iteration 705, loss = 0.03327682\n",
      "Iteration 706, loss = 0.03314865\n",
      "Iteration 707, loss = 0.03299370\n",
      "Iteration 708, loss = 0.03289459\n",
      "Iteration 709, loss = 0.03278001\n",
      "Iteration 710, loss = 0.03263897\n",
      "Iteration 711, loss = 0.03253546\n",
      "Iteration 712, loss = 0.03239078\n",
      "Iteration 713, loss = 0.03226787\n",
      "Iteration 714, loss = 0.03215210\n",
      "Iteration 715, loss = 0.03206126\n",
      "Iteration 716, loss = 0.03191302\n",
      "Iteration 717, loss = 0.03183175\n",
      "Iteration 718, loss = 0.03167598\n",
      "Iteration 719, loss = 0.03156879\n",
      "Iteration 720, loss = 0.03144201\n",
      "Iteration 721, loss = 0.03135813\n",
      "Iteration 722, loss = 0.03121539\n",
      "Iteration 723, loss = 0.03113669\n",
      "Iteration 724, loss = 0.03098040\n",
      "Iteration 725, loss = 0.03086119\n",
      "Iteration 726, loss = 0.03076182\n",
      "Iteration 727, loss = 0.03064795\n",
      "Iteration 728, loss = 0.03056061\n",
      "Iteration 729, loss = 0.03039530\n",
      "Iteration 730, loss = 0.03032306\n",
      "Iteration 731, loss = 0.03020408\n",
      "Iteration 732, loss = 0.03007868\n",
      "Iteration 733, loss = 0.02999910\n",
      "Iteration 734, loss = 0.02988832\n",
      "Iteration 735, loss = 0.02974603\n",
      "Iteration 736, loss = 0.02963545\n",
      "Iteration 737, loss = 0.02952184\n",
      "Iteration 738, loss = 0.02940579\n",
      "Iteration 739, loss = 0.02931161\n",
      "Iteration 740, loss = 0.02923886\n",
      "Iteration 741, loss = 0.02909337\n",
      "Iteration 742, loss = 0.02900865\n",
      "Iteration 743, loss = 0.02889873\n",
      "Iteration 744, loss = 0.02879706\n",
      "Iteration 745, loss = 0.02867933\n",
      "Iteration 746, loss = 0.02858078\n",
      "Iteration 747, loss = 0.02848174\n",
      "Iteration 748, loss = 0.02836317\n",
      "Iteration 749, loss = 0.02827521\n",
      "Iteration 750, loss = 0.02817715\n",
      "Iteration 751, loss = 0.02806695\n",
      "Iteration 752, loss = 0.02797245\n",
      "Iteration 753, loss = 0.02787261\n",
      "Iteration 754, loss = 0.02776717\n",
      "Iteration 755, loss = 0.02767618\n",
      "Iteration 756, loss = 0.02757732\n",
      "Iteration 757, loss = 0.02752043\n",
      "Iteration 758, loss = 0.02741821\n",
      "Iteration 759, loss = 0.02727831\n",
      "Iteration 760, loss = 0.02722850\n",
      "Iteration 761, loss = 0.02708397\n",
      "Iteration 762, loss = 0.02701132\n",
      "Iteration 763, loss = 0.02689474\n",
      "Iteration 764, loss = 0.02681028\n",
      "Iteration 765, loss = 0.02668727\n",
      "Iteration 766, loss = 0.02660456\n",
      "Iteration 767, loss = 0.02650987\n",
      "Iteration 768, loss = 0.02640962\n",
      "Iteration 769, loss = 0.02633065\n",
      "Iteration 770, loss = 0.02623802\n",
      "Iteration 771, loss = 0.02612551\n",
      "Iteration 772, loss = 0.02607371\n",
      "Iteration 773, loss = 0.02597869\n",
      "Iteration 774, loss = 0.02586946\n",
      "Iteration 775, loss = 0.02579808\n",
      "Iteration 776, loss = 0.02567613\n",
      "Iteration 777, loss = 0.02558725\n",
      "Iteration 778, loss = 0.02555169\n",
      "Iteration 779, loss = 0.02543458\n",
      "Iteration 780, loss = 0.02534516\n",
      "Iteration 781, loss = 0.02531272\n",
      "Iteration 782, loss = 0.02521825\n",
      "Iteration 783, loss = 0.02506626\n",
      "Iteration 784, loss = 0.02498110\n",
      "Iteration 785, loss = 0.02489751\n",
      "Iteration 786, loss = 0.02480150\n",
      "Iteration 787, loss = 0.02472083\n",
      "Iteration 788, loss = 0.02462880\n",
      "Iteration 789, loss = 0.02455528\n",
      "Iteration 790, loss = 0.02448607\n",
      "Iteration 791, loss = 0.02442506\n",
      "Iteration 792, loss = 0.02431950\n",
      "Iteration 793, loss = 0.02425311\n",
      "Iteration 794, loss = 0.02412728\n",
      "Iteration 795, loss = 0.02406162\n",
      "Iteration 796, loss = 0.02396587\n",
      "Iteration 797, loss = 0.02389270\n",
      "Iteration 798, loss = 0.02381532\n",
      "Iteration 799, loss = 0.02374656\n",
      "Iteration 800, loss = 0.02363354\n",
      "Iteration 801, loss = 0.02355984\n",
      "Iteration 802, loss = 0.02348161\n",
      "Iteration 803, loss = 0.02339634\n",
      "Iteration 804, loss = 0.02332378\n",
      "Iteration 805, loss = 0.02324689\n",
      "Iteration 806, loss = 0.02314696\n",
      "Iteration 807, loss = 0.02308204\n",
      "Iteration 808, loss = 0.02301575\n",
      "Iteration 809, loss = 0.02295128\n",
      "Iteration 810, loss = 0.02282359\n",
      "Iteration 811, loss = 0.02279506\n",
      "Iteration 812, loss = 0.02270639\n",
      "Iteration 813, loss = 0.02260345\n",
      "Iteration 814, loss = 0.02252848\n",
      "Iteration 815, loss = 0.02247679\n",
      "Iteration 816, loss = 0.02239874\n",
      "Iteration 817, loss = 0.02230501\n",
      "Iteration 818, loss = 0.02224285\n",
      "Iteration 819, loss = 0.02217939\n",
      "Iteration 820, loss = 0.02211029\n",
      "Iteration 821, loss = 0.02202452\n",
      "Iteration 822, loss = 0.02195488\n",
      "Iteration 823, loss = 0.02190263\n",
      "Iteration 824, loss = 0.02178312\n",
      "Iteration 825, loss = 0.02174099\n",
      "Iteration 826, loss = 0.02168152\n",
      "Iteration 827, loss = 0.02158720\n",
      "Iteration 828, loss = 0.02151507\n",
      "Iteration 829, loss = 0.02144358\n",
      "Iteration 830, loss = 0.02136669\n",
      "Iteration 831, loss = 0.02130152\n",
      "Iteration 832, loss = 0.02124733\n",
      "Iteration 833, loss = 0.02115986\n",
      "Iteration 834, loss = 0.02108975\n",
      "Iteration 835, loss = 0.02100756\n",
      "Iteration 836, loss = 0.02094539\n",
      "Iteration 837, loss = 0.02091958\n",
      "Iteration 838, loss = 0.02080580\n",
      "Iteration 839, loss = 0.02074304\n",
      "Iteration 840, loss = 0.02068056\n",
      "Iteration 841, loss = 0.02061735\n",
      "Iteration 842, loss = 0.02054835\n",
      "Iteration 843, loss = 0.02047897\n",
      "Iteration 844, loss = 0.02043346\n",
      "Iteration 845, loss = 0.02035196\n",
      "Iteration 846, loss = 0.02026388\n",
      "Iteration 847, loss = 0.02020115\n",
      "Iteration 848, loss = 0.02015717\n",
      "Iteration 849, loss = 0.02008668\n",
      "Iteration 850, loss = 0.02003930\n",
      "Iteration 851, loss = 0.01999782\n",
      "Iteration 852, loss = 0.01989421\n",
      "Iteration 853, loss = 0.01982596\n",
      "Iteration 854, loss = 0.01976446\n",
      "Iteration 855, loss = 0.01971610\n",
      "Iteration 856, loss = 0.01964287\n",
      "Iteration 857, loss = 0.01958272\n",
      "Iteration 858, loss = 0.01952898\n",
      "Iteration 859, loss = 0.01956228\n",
      "Iteration 860, loss = 0.01944908\n",
      "Iteration 861, loss = 0.01932936\n",
      "Iteration 862, loss = 0.01930030\n",
      "Iteration 863, loss = 0.01921060\n",
      "Iteration 864, loss = 0.01915596\n",
      "Iteration 865, loss = 0.01911694\n",
      "Iteration 866, loss = 0.01902915\n",
      "Iteration 867, loss = 0.01897557\n",
      "Iteration 868, loss = 0.01889183\n",
      "Iteration 869, loss = 0.01883905\n",
      "Iteration 870, loss = 0.01879317\n",
      "Iteration 871, loss = 0.01872610\n",
      "Iteration 872, loss = 0.01866244\n",
      "Iteration 873, loss = 0.01861196\n",
      "Iteration 874, loss = 0.01857818\n",
      "Iteration 875, loss = 0.01847418\n",
      "Iteration 876, loss = 0.01842664\n",
      "Iteration 877, loss = 0.01841033\n",
      "Iteration 878, loss = 0.01833195\n",
      "Iteration 879, loss = 0.01829016\n",
      "Iteration 880, loss = 0.01821658\n",
      "Iteration 881, loss = 0.01813005\n",
      "Iteration 882, loss = 0.01808145\n",
      "Iteration 883, loss = 0.01805811\n",
      "Iteration 884, loss = 0.01804090\n",
      "Iteration 885, loss = 0.01791793\n",
      "Iteration 886, loss = 0.01789588\n",
      "Iteration 887, loss = 0.01782212\n",
      "Iteration 888, loss = 0.01774748\n",
      "Iteration 889, loss = 0.01770568\n",
      "Iteration 890, loss = 0.01764885\n",
      "Iteration 891, loss = 0.01759112\n",
      "Iteration 892, loss = 0.01754521\n",
      "Iteration 893, loss = 0.01748540\n",
      "Iteration 894, loss = 0.01743790\n",
      "Iteration 895, loss = 0.01742102\n",
      "Iteration 896, loss = 0.01730520\n",
      "Iteration 897, loss = 0.01729981\n",
      "Iteration 898, loss = 0.01720322\n",
      "Iteration 899, loss = 0.01715881\n",
      "Iteration 900, loss = 0.01718541\n",
      "Iteration 901, loss = 0.01703688\n",
      "Iteration 902, loss = 0.01701510\n",
      "Iteration 903, loss = 0.01702443\n",
      "Iteration 904, loss = 0.01694069\n",
      "Iteration 905, loss = 0.01686596\n",
      "Iteration 906, loss = 0.01680284\n",
      "Iteration 907, loss = 0.01675975\n",
      "Iteration 908, loss = 0.01668488\n",
      "Iteration 909, loss = 0.01665737\n",
      "Iteration 910, loss = 0.01659967\n",
      "Iteration 911, loss = 0.01655815\n",
      "Iteration 912, loss = 0.01648850\n",
      "Iteration 913, loss = 0.01645193\n",
      "Iteration 914, loss = 0.01642143\n",
      "Iteration 915, loss = 0.01637125\n",
      "Iteration 916, loss = 0.01629582\n",
      "Iteration 917, loss = 0.01625469\n",
      "Iteration 918, loss = 0.01620368\n",
      "Iteration 919, loss = 0.01620921\n",
      "Iteration 920, loss = 0.01612934\n",
      "Iteration 921, loss = 0.01605132\n",
      "Iteration 922, loss = 0.01600330\n",
      "Iteration 923, loss = 0.01595731\n",
      "Iteration 924, loss = 0.01592214\n",
      "Iteration 925, loss = 0.01589140\n",
      "Iteration 926, loss = 0.01584804\n",
      "Iteration 927, loss = 0.01578404\n",
      "Iteration 928, loss = 0.01575743\n",
      "Iteration 929, loss = 0.01569029\n",
      "Iteration 930, loss = 0.01565623\n",
      "Iteration 931, loss = 0.01559368\n",
      "Iteration 932, loss = 0.01553372\n",
      "Iteration 933, loss = 0.01552330\n",
      "Iteration 934, loss = 0.01550031\n",
      "Iteration 935, loss = 0.01542080\n",
      "Iteration 936, loss = 0.01537390\n",
      "Iteration 937, loss = 0.01531775\n",
      "Iteration 938, loss = 0.01529359\n",
      "Iteration 939, loss = 0.01524101\n",
      "Iteration 940, loss = 0.01519192\n",
      "Iteration 941, loss = 0.01515630\n",
      "Iteration 942, loss = 0.01510074\n",
      "Iteration 943, loss = 0.01506619\n",
      "Iteration 944, loss = 0.01501134\n",
      "Iteration 945, loss = 0.01498776\n",
      "Iteration 946, loss = 0.01495364\n",
      "Iteration 947, loss = 0.01489490\n",
      "Iteration 948, loss = 0.01484952\n",
      "Iteration 949, loss = 0.01479496\n",
      "Iteration 950, loss = 0.01476347\n",
      "Iteration 951, loss = 0.01475086\n",
      "Iteration 952, loss = 0.01466573\n",
      "Iteration 953, loss = 0.01463948\n",
      "Iteration 954, loss = 0.01460938\n",
      "Iteration 955, loss = 0.01454127\n",
      "Iteration 956, loss = 0.01450675\n",
      "Iteration 957, loss = 0.01450075\n",
      "Iteration 958, loss = 0.01442935\n",
      "Iteration 959, loss = 0.01438175\n",
      "Iteration 960, loss = 0.01436594\n",
      "Iteration 961, loss = 0.01431259\n",
      "Iteration 962, loss = 0.01426249\n",
      "Iteration 963, loss = 0.01422310\n",
      "Iteration 964, loss = 0.01417870\n",
      "Iteration 965, loss = 0.01414708\n",
      "Iteration 966, loss = 0.01413231\n",
      "Iteration 967, loss = 0.01404179\n",
      "Iteration 968, loss = 0.01403978\n",
      "Iteration 969, loss = 0.01399185\n",
      "Iteration 970, loss = 0.01397687\n",
      "Iteration 971, loss = 0.01392384\n",
      "Iteration 972, loss = 0.01384801\n",
      "Iteration 973, loss = 0.01383104\n",
      "Iteration 974, loss = 0.01379154\n",
      "Iteration 975, loss = 0.01376565\n",
      "Iteration 976, loss = 0.01371216\n",
      "Iteration 977, loss = 0.01367487\n",
      "Iteration 978, loss = 0.01365019\n",
      "Iteration 979, loss = 0.01359927\n",
      "Iteration 980, loss = 0.01356513\n",
      "Iteration 981, loss = 0.01354418\n",
      "Iteration 982, loss = 0.01348620\n",
      "Iteration 983, loss = 0.01350670\n",
      "Iteration 984, loss = 0.01341529\n",
      "Iteration 985, loss = 0.01338916\n",
      "Iteration 986, loss = 0.01337324\n",
      "Iteration 987, loss = 0.01330887\n",
      "Iteration 988, loss = 0.01326622\n",
      "Iteration 989, loss = 0.01322464\n",
      "Iteration 990, loss = 0.01322766\n",
      "Iteration 991, loss = 0.01316368\n",
      "Iteration 992, loss = 0.01314720\n",
      "Iteration 993, loss = 0.01311237\n",
      "Iteration 994, loss = 0.01307968\n",
      "Iteration 995, loss = 0.01300138\n",
      "Iteration 996, loss = 0.01301158\n",
      "Iteration 997, loss = 0.01293875\n",
      "Iteration 998, loss = 0.01290540\n",
      "Iteration 999, loss = 0.01289676\n",
      "Iteration 1000, loss = 0.01284180\n",
      "Iteration 1001, loss = 0.01281827\n",
      "Iteration 1002, loss = 0.01276172\n",
      "Iteration 1003, loss = 0.01274396\n",
      "Iteration 1004, loss = 0.01269842\n",
      "Iteration 1005, loss = 0.01267866\n",
      "Iteration 1006, loss = 0.01264431\n",
      "Iteration 1007, loss = 0.01259537\n",
      "Iteration 1008, loss = 0.01257422\n",
      "Iteration 1009, loss = 0.01253087\n",
      "Iteration 1010, loss = 0.01250489\n",
      "Iteration 1011, loss = 0.01248410\n",
      "Iteration 1012, loss = 0.01247089\n",
      "Iteration 1013, loss = 0.01242987\n",
      "Iteration 1014, loss = 0.01236918\n",
      "Iteration 1015, loss = 0.01238058\n",
      "Iteration 1016, loss = 0.01229409\n",
      "Iteration 1017, loss = 0.01228736\n",
      "Iteration 1018, loss = 0.01228507\n",
      "Iteration 1019, loss = 0.01221542\n",
      "Iteration 1020, loss = 0.01218618\n",
      "Iteration 1021, loss = 0.01214482\n",
      "Iteration 1022, loss = 0.01211601\n",
      "Iteration 1023, loss = 0.01208172\n",
      "Iteration 1024, loss = 0.01206571\n",
      "Iteration 1025, loss = 0.01201271\n",
      "Iteration 1026, loss = 0.01197331\n",
      "Iteration 1027, loss = 0.01196220\n",
      "Iteration 1028, loss = 0.01193424\n",
      "Iteration 1029, loss = 0.01190929\n",
      "Iteration 1030, loss = 0.01190678\n",
      "Iteration 1031, loss = 0.01185668\n",
      "Iteration 1032, loss = 0.01179944\n",
      "Iteration 1033, loss = 0.01179363\n",
      "Iteration 1034, loss = 0.01174103\n",
      "Iteration 1035, loss = 0.01171021\n",
      "Iteration 1036, loss = 0.01168540\n",
      "Iteration 1037, loss = 0.01166703\n",
      "Iteration 1038, loss = 0.01161126\n",
      "Iteration 1039, loss = 0.01157976\n",
      "Iteration 1040, loss = 0.01155167\n",
      "Iteration 1041, loss = 0.01153914\n",
      "Iteration 1042, loss = 0.01154757\n",
      "Iteration 1043, loss = 0.01149224\n",
      "Iteration 1044, loss = 0.01149096\n",
      "Iteration 1045, loss = 0.01140735\n",
      "Iteration 1046, loss = 0.01137942\n",
      "Iteration 1047, loss = 0.01134883\n",
      "Iteration 1048, loss = 0.01133635\n",
      "Iteration 1049, loss = 0.01130797\n",
      "Iteration 1050, loss = 0.01126717\n",
      "Iteration 1051, loss = 0.01125289\n",
      "Iteration 1052, loss = 0.01122794\n",
      "Iteration 1053, loss = 0.01117425\n",
      "Iteration 1054, loss = 0.01117769\n",
      "Iteration 1055, loss = 0.01113655\n",
      "Iteration 1056, loss = 0.01112833\n",
      "Iteration 1057, loss = 0.01107345\n",
      "Iteration 1058, loss = 0.01102597\n",
      "Iteration 1059, loss = 0.01101428\n",
      "Iteration 1060, loss = 0.01098638\n",
      "Iteration 1061, loss = 0.01095743\n",
      "Iteration 1062, loss = 0.01093872\n",
      "Iteration 1063, loss = 0.01093231\n",
      "Iteration 1064, loss = 0.01086762\n",
      "Iteration 1065, loss = 0.01083714\n",
      "Iteration 1066, loss = 0.01084741\n",
      "Iteration 1067, loss = 0.01078842\n",
      "Iteration 1068, loss = 0.01076026\n",
      "Iteration 1069, loss = 0.01074378\n",
      "Iteration 1070, loss = 0.01072945\n",
      "Iteration 1071, loss = 0.01069855\n",
      "Iteration 1072, loss = 0.01068763\n",
      "Iteration 1073, loss = 0.01064577\n",
      "Iteration 1074, loss = 0.01059606\n",
      "Iteration 1075, loss = 0.01056982\n",
      "Iteration 1076, loss = 0.01054889\n",
      "Iteration 1077, loss = 0.01052880\n",
      "Iteration 1078, loss = 0.01051936\n",
      "Iteration 1079, loss = 0.01049354\n",
      "Iteration 1080, loss = 0.01043857\n",
      "Iteration 1081, loss = 0.01044075\n",
      "Iteration 1082, loss = 0.01039455\n",
      "Iteration 1083, loss = 0.01038776\n",
      "Iteration 1084, loss = 0.01033723\n",
      "Iteration 1085, loss = 0.01030671\n",
      "Iteration 1086, loss = 0.01031244\n",
      "Iteration 1087, loss = 0.01027165\n",
      "Iteration 1088, loss = 0.01027191\n",
      "Iteration 1089, loss = 0.01021011\n",
      "Iteration 1090, loss = 0.01018200\n",
      "Iteration 1091, loss = 0.01017213\n",
      "Iteration 1092, loss = 0.01016847\n",
      "Iteration 1093, loss = 0.01010200\n",
      "Iteration 1094, loss = 0.01009109\n",
      "Iteration 1095, loss = 0.01008932\n",
      "Iteration 1096, loss = 0.01002575\n",
      "Iteration 1097, loss = 0.01000662\n",
      "Iteration 1098, loss = 0.00998532\n",
      "Iteration 1099, loss = 0.00998213\n",
      "Iteration 1100, loss = 0.00996858\n",
      "Iteration 1101, loss = 0.00990815\n",
      "Iteration 1102, loss = 0.00990790\n",
      "Iteration 1103, loss = 0.00986738\n",
      "Iteration 1104, loss = 0.00986696\n",
      "Iteration 1105, loss = 0.00984456\n",
      "Iteration 1106, loss = 0.00980165\n",
      "Iteration 1107, loss = 0.00978496\n",
      "Iteration 1108, loss = 0.00977187\n",
      "Iteration 1109, loss = 0.00972313\n",
      "Iteration 1110, loss = 0.00971982\n",
      "Iteration 1111, loss = 0.00969278\n",
      "Iteration 1112, loss = 0.00965139\n",
      "Iteration 1113, loss = 0.00967708\n",
      "Iteration 1114, loss = 0.00959772\n",
      "Iteration 1115, loss = 0.00958153\n",
      "Iteration 1116, loss = 0.00955704\n",
      "Iteration 1117, loss = 0.00954991\n",
      "Iteration 1118, loss = 0.00952468\n",
      "Iteration 1119, loss = 0.00949117\n",
      "Iteration 1120, loss = 0.00953886\n",
      "Iteration 1121, loss = 0.00944880\n",
      "Iteration 1122, loss = 0.00942284\n",
      "Iteration 1123, loss = 0.00940258\n",
      "Iteration 1124, loss = 0.00941569\n",
      "Iteration 1125, loss = 0.00936089\n",
      "Iteration 1126, loss = 0.00933990\n",
      "Iteration 1127, loss = 0.00931980\n",
      "Iteration 1128, loss = 0.00929176\n",
      "Iteration 1129, loss = 0.00927013\n",
      "Iteration 1130, loss = 0.00924256\n",
      "Iteration 1131, loss = 0.00924696\n",
      "Iteration 1132, loss = 0.00919991\n",
      "Iteration 1133, loss = 0.00918532\n",
      "Iteration 1134, loss = 0.00916276\n",
      "Iteration 1135, loss = 0.00914653\n",
      "Iteration 1136, loss = 0.00910297\n",
      "Iteration 1137, loss = 0.00911659\n",
      "Iteration 1138, loss = 0.00911125\n",
      "Iteration 1139, loss = 0.00907783\n",
      "Iteration 1140, loss = 0.00903045\n",
      "Iteration 1141, loss = 0.00900958\n",
      "Iteration 1142, loss = 0.00899977\n",
      "Iteration 1143, loss = 0.00895755\n",
      "Iteration 1144, loss = 0.00895065\n",
      "Iteration 1145, loss = 0.00893469\n",
      "Iteration 1146, loss = 0.00891973\n",
      "Iteration 1147, loss = 0.00890733\n",
      "Iteration 1148, loss = 0.00889814\n",
      "Iteration 1149, loss = 0.00884258\n",
      "Iteration 1150, loss = 0.00882067\n",
      "Iteration 1151, loss = 0.00880602\n",
      "Iteration 1152, loss = 0.00879087\n",
      "Iteration 1153, loss = 0.00875879\n",
      "Iteration 1154, loss = 0.00874267\n",
      "Iteration 1155, loss = 0.00875584\n",
      "Iteration 1156, loss = 0.00870342\n",
      "Iteration 1157, loss = 0.00868475\n",
      "Iteration 1158, loss = 0.00867852\n",
      "Iteration 1159, loss = 0.00869484\n",
      "Iteration 1160, loss = 0.00864646\n",
      "Iteration 1161, loss = 0.00860234\n",
      "Iteration 1162, loss = 0.00859100\n",
      "Iteration 1163, loss = 0.00856861\n",
      "Iteration 1164, loss = 0.00856094\n",
      "Iteration 1165, loss = 0.00852498\n",
      "Iteration 1166, loss = 0.00852637\n",
      "Iteration 1167, loss = 0.00850415\n",
      "Iteration 1168, loss = 0.00849279\n",
      "Iteration 1169, loss = 0.00847510\n",
      "Iteration 1170, loss = 0.00848208\n",
      "Iteration 1171, loss = 0.00841500\n",
      "Iteration 1172, loss = 0.00839554\n",
      "Iteration 1173, loss = 0.00839205\n",
      "Iteration 1174, loss = 0.00836910\n",
      "Iteration 1175, loss = 0.00833165\n",
      "Iteration 1176, loss = 0.00832526\n",
      "Iteration 1177, loss = 0.00829410\n",
      "Iteration 1178, loss = 0.00828624\n",
      "Iteration 1179, loss = 0.00826728\n",
      "Iteration 1180, loss = 0.00824466\n",
      "Iteration 1181, loss = 0.00823285\n",
      "Iteration 1182, loss = 0.00821123\n",
      "Iteration 1183, loss = 0.00819310\n",
      "Iteration 1184, loss = 0.00816159\n",
      "Iteration 1185, loss = 0.00814406\n",
      "Iteration 1186, loss = 0.00814563\n",
      "Iteration 1187, loss = 0.00811455\n",
      "Iteration 1188, loss = 0.00810240\n",
      "Iteration 1189, loss = 0.00808779\n",
      "Iteration 1190, loss = 0.00806504\n",
      "Iteration 1191, loss = 0.00805468\n",
      "Iteration 1192, loss = 0.00802946\n",
      "Iteration 1193, loss = 0.00800376\n",
      "Iteration 1194, loss = 0.00799641\n",
      "Iteration 1195, loss = 0.00799152\n",
      "Iteration 1196, loss = 0.00796536\n",
      "Iteration 1197, loss = 0.00793674\n",
      "Iteration 1198, loss = 0.00795432\n",
      "Iteration 1199, loss = 0.00790129\n",
      "Iteration 1200, loss = 0.00789524\n",
      "Iteration 1201, loss = 0.00786022\n",
      "Iteration 1202, loss = 0.00785459\n",
      "Iteration 1203, loss = 0.00783463\n",
      "Iteration 1204, loss = 0.00781834\n",
      "Iteration 1205, loss = 0.00779562\n",
      "Iteration 1206, loss = 0.00779207\n",
      "Iteration 1207, loss = 0.00777730\n",
      "Iteration 1208, loss = 0.00776710\n",
      "Iteration 1209, loss = 0.00773081\n",
      "Iteration 1210, loss = 0.00771888\n",
      "Iteration 1211, loss = 0.00770070\n",
      "Iteration 1212, loss = 0.00769326\n",
      "Iteration 1213, loss = 0.00767282\n",
      "Iteration 1214, loss = 0.00766494\n",
      "Iteration 1215, loss = 0.00765352\n",
      "Iteration 1216, loss = 0.00762576\n",
      "Iteration 1217, loss = 0.00760253\n",
      "Iteration 1218, loss = 0.00760871\n",
      "Iteration 1219, loss = 0.00757410\n",
      "Iteration 1220, loss = 0.00755289\n",
      "Iteration 1221, loss = 0.00755245\n",
      "Iteration 1222, loss = 0.00752043\n",
      "Iteration 1223, loss = 0.00750346\n",
      "Iteration 1224, loss = 0.00749612\n",
      "Iteration 1225, loss = 0.00748325\n",
      "Iteration 1226, loss = 0.00747128\n",
      "Iteration 1227, loss = 0.00742965\n",
      "Iteration 1228, loss = 0.00743105\n",
      "Iteration 1229, loss = 0.00741084\n",
      "Iteration 1230, loss = 0.00739857\n",
      "Iteration 1231, loss = 0.00737949\n",
      "Iteration 1232, loss = 0.00738758\n",
      "Iteration 1233, loss = 0.00735702\n",
      "Iteration 1234, loss = 0.00732259\n",
      "Iteration 1235, loss = 0.00730980\n",
      "Iteration 1236, loss = 0.00728788\n",
      "Iteration 1237, loss = 0.00728553\n",
      "Iteration 1238, loss = 0.00728742\n",
      "Iteration 1239, loss = 0.00725575\n",
      "Iteration 1240, loss = 0.00723812\n",
      "Iteration 1241, loss = 0.00722647\n",
      "Iteration 1242, loss = 0.00721235\n",
      "Iteration 1243, loss = 0.00718445\n",
      "Iteration 1244, loss = 0.00718972\n",
      "Iteration 1245, loss = 0.00716396\n",
      "Iteration 1246, loss = 0.00715779\n",
      "Iteration 1247, loss = 0.00713630\n",
      "Iteration 1248, loss = 0.00711721\n",
      "Iteration 1249, loss = 0.00710657\n",
      "Iteration 1250, loss = 0.00707932\n",
      "Iteration 1251, loss = 0.00708606\n",
      "Iteration 1252, loss = 0.00705768\n",
      "Iteration 1253, loss = 0.00708017\n",
      "Iteration 1254, loss = 0.00704752\n",
      "Iteration 1255, loss = 0.00700136\n",
      "Iteration 1256, loss = 0.00700493\n",
      "Iteration 1257, loss = 0.00698081\n",
      "Iteration 1258, loss = 0.00696688\n",
      "Iteration 1259, loss = 0.00695423\n",
      "Iteration 1260, loss = 0.00697138\n",
      "Iteration 1261, loss = 0.00695831\n",
      "Iteration 1262, loss = 0.00692125\n",
      "Iteration 1263, loss = 0.00690725\n",
      "Iteration 1264, loss = 0.00688524\n",
      "Iteration 1265, loss = 0.00688832\n",
      "Iteration 1266, loss = 0.00688539\n",
      "Iteration 1267, loss = 0.00685206\n",
      "Iteration 1268, loss = 0.00685231\n",
      "Iteration 1269, loss = 0.00685541\n",
      "Iteration 1270, loss = 0.00682294\n",
      "Iteration 1271, loss = 0.00680296\n",
      "Iteration 1272, loss = 0.00677205\n",
      "Iteration 1273, loss = 0.00677383\n",
      "Iteration 1274, loss = 0.00675535\n",
      "Iteration 1275, loss = 0.00673684\n",
      "Iteration 1276, loss = 0.00671625\n",
      "Iteration 1277, loss = 0.00672816\n",
      "Iteration 1278, loss = 0.00669144\n",
      "Iteration 1279, loss = 0.00669278\n",
      "Iteration 1280, loss = 0.00666319\n",
      "Iteration 1281, loss = 0.00664576\n",
      "Iteration 1282, loss = 0.00664317\n",
      "Iteration 1283, loss = 0.00663062\n",
      "Iteration 1284, loss = 0.00664146\n",
      "Iteration 1285, loss = 0.00662619\n",
      "Iteration 1286, loss = 0.00659381\n",
      "Iteration 1287, loss = 0.00658895\n",
      "Iteration 1288, loss = 0.00657348\n",
      "Iteration 1289, loss = 0.00655682\n",
      "Iteration 1290, loss = 0.00653562\n",
      "Iteration 1291, loss = 0.00654931\n",
      "Iteration 1292, loss = 0.00649885\n",
      "Iteration 1293, loss = 0.00650102\n",
      "Iteration 1294, loss = 0.00647000\n",
      "Iteration 1295, loss = 0.00647068\n",
      "Iteration 1296, loss = 0.00645515\n",
      "Iteration 1297, loss = 0.00645014\n",
      "Iteration 1298, loss = 0.00643102\n",
      "Iteration 1299, loss = 0.00641595\n",
      "Iteration 1300, loss = 0.00640284\n",
      "Iteration 1301, loss = 0.00639318\n",
      "Iteration 1302, loss = 0.00637827\n",
      "Iteration 1303, loss = 0.00636542\n",
      "Iteration 1304, loss = 0.00633835\n",
      "Iteration 1305, loss = 0.00633579\n",
      "Iteration 1306, loss = 0.00635583\n",
      "Iteration 1307, loss = 0.00630422\n",
      "Iteration 1308, loss = 0.00629847\n",
      "Iteration 1309, loss = 0.00628897\n",
      "Iteration 1310, loss = 0.00627893\n",
      "Iteration 1311, loss = 0.00629335\n",
      "Iteration 1312, loss = 0.00625947\n",
      "Iteration 1313, loss = 0.00625221\n",
      "Iteration 1314, loss = 0.00622347\n",
      "Iteration 1315, loss = 0.00621644\n",
      "Iteration 1316, loss = 0.00621244\n",
      "Iteration 1317, loss = 0.00620487\n",
      "Iteration 1318, loss = 0.00618630\n",
      "Iteration 1319, loss = 0.00617808\n",
      "Iteration 1320, loss = 0.00617517\n",
      "Iteration 1321, loss = 0.00612917\n",
      "Iteration 1322, loss = 0.00613105\n",
      "Iteration 1323, loss = 0.00613329\n",
      "Iteration 1324, loss = 0.00612195\n",
      "Iteration 1325, loss = 0.00612353\n",
      "Iteration 1326, loss = 0.00609479\n",
      "Iteration 1327, loss = 0.00607753\n",
      "Iteration 1328, loss = 0.00606452\n",
      "Iteration 1329, loss = 0.00611132\n",
      "Iteration 1330, loss = 0.00605221\n",
      "Iteration 1331, loss = 0.00602602\n",
      "Iteration 1332, loss = 0.00604620\n",
      "Iteration 1333, loss = 0.00599677\n",
      "Iteration 1334, loss = 0.00600040\n",
      "Iteration 1335, loss = 0.00598572\n",
      "Iteration 1336, loss = 0.00598032\n",
      "Iteration 1337, loss = 0.00595334\n",
      "Iteration 1338, loss = 0.00596826\n",
      "Iteration 1339, loss = 0.00592848\n",
      "Iteration 1340, loss = 0.00592829\n",
      "Iteration 1341, loss = 0.00591167\n",
      "Iteration 1342, loss = 0.00589398\n",
      "Iteration 1343, loss = 0.00588932\n",
      "Iteration 1344, loss = 0.00588265\n",
      "Iteration 1345, loss = 0.00593779\n",
      "Iteration 1346, loss = 0.00585162\n",
      "Iteration 1347, loss = 0.00584476\n",
      "Iteration 1348, loss = 0.00583233\n",
      "Iteration 1349, loss = 0.00583668\n",
      "Iteration 1350, loss = 0.00583772\n",
      "Iteration 1351, loss = 0.00580734\n",
      "Iteration 1352, loss = 0.00577723\n",
      "Iteration 1353, loss = 0.00578673\n",
      "Iteration 1354, loss = 0.00579511\n",
      "Iteration 1355, loss = 0.00578941\n",
      "Iteration 1356, loss = 0.00575935\n",
      "Iteration 1357, loss = 0.00574692\n",
      "Iteration 1358, loss = 0.00573345\n",
      "Iteration 1359, loss = 0.00570351\n",
      "Iteration 1360, loss = 0.00570593\n",
      "Iteration 1361, loss = 0.00568868\n",
      "Iteration 1362, loss = 0.00568114\n",
      "Iteration 1363, loss = 0.00566892\n",
      "Iteration 1364, loss = 0.00565691\n",
      "Iteration 1365, loss = 0.00565982\n",
      "Iteration 1366, loss = 0.00564003\n",
      "Iteration 1367, loss = 0.00562914\n",
      "Iteration 1368, loss = 0.00561983\n",
      "Iteration 1369, loss = 0.00561147\n",
      "Iteration 1370, loss = 0.00561586\n",
      "Iteration 1371, loss = 0.00559927\n",
      "Iteration 1372, loss = 0.00558482\n",
      "Iteration 1373, loss = 0.00556030\n",
      "Iteration 1374, loss = 0.00555184\n",
      "Iteration 1375, loss = 0.00554415\n",
      "Iteration 1376, loss = 0.00554533\n",
      "Iteration 1377, loss = 0.00553043\n",
      "Iteration 1378, loss = 0.00552031\n",
      "Iteration 1379, loss = 0.00551223\n",
      "Iteration 1380, loss = 0.00549192\n",
      "Iteration 1381, loss = 0.00550446\n",
      "Iteration 1382, loss = 0.00547179\n",
      "Iteration 1383, loss = 0.00548094\n",
      "Iteration 1384, loss = 0.00544552\n",
      "Iteration 1385, loss = 0.00545908\n",
      "Iteration 1386, loss = 0.00544027\n",
      "Iteration 1387, loss = 0.00543127\n",
      "Iteration 1388, loss = 0.00542109\n",
      "Iteration 1389, loss = 0.00541594\n",
      "Iteration 1390, loss = 0.00540438\n",
      "Iteration 1391, loss = 0.00541870\n",
      "Iteration 1392, loss = 0.00538616\n",
      "Iteration 1393, loss = 0.00536400\n",
      "Iteration 1394, loss = 0.00535385\n",
      "Iteration 1395, loss = 0.00535289\n",
      "Iteration 1396, loss = 0.00534180\n",
      "Iteration 1397, loss = 0.00533253\n",
      "Iteration 1398, loss = 0.00532225\n",
      "Iteration 1399, loss = 0.00532068\n",
      "Iteration 1400, loss = 0.00533706\n",
      "Iteration 1401, loss = 0.00528810\n",
      "Iteration 1402, loss = 0.00528122\n",
      "Iteration 1403, loss = 0.00528743\n",
      "Iteration 1404, loss = 0.00527213\n",
      "Iteration 1405, loss = 0.00528178\n",
      "Iteration 1406, loss = 0.00533671\n",
      "Iteration 1407, loss = 0.00525135\n",
      "Iteration 1408, loss = 0.00524178\n",
      "Iteration 1409, loss = 0.00521907\n",
      "Iteration 1410, loss = 0.00522452\n",
      "Iteration 1411, loss = 0.00521003\n",
      "Iteration 1412, loss = 0.00520734\n",
      "Iteration 1413, loss = 0.00519316\n",
      "Iteration 1414, loss = 0.00516945\n",
      "Iteration 1415, loss = 0.00516673\n",
      "Iteration 1416, loss = 0.00517234\n",
      "Iteration 1417, loss = 0.00513918\n",
      "Iteration 1418, loss = 0.00513395\n",
      "Iteration 1419, loss = 0.00513917\n",
      "Iteration 1420, loss = 0.00511984\n",
      "Iteration 1421, loss = 0.00510777\n",
      "Iteration 1422, loss = 0.00510807\n",
      "Iteration 1423, loss = 0.00509475\n",
      "Iteration 1424, loss = 0.00508295\n",
      "Iteration 1425, loss = 0.00508219\n",
      "Iteration 1426, loss = 0.00507573\n",
      "Iteration 1427, loss = 0.00505646\n",
      "Iteration 1428, loss = 0.00505014\n",
      "Iteration 1429, loss = 0.00503394\n",
      "Iteration 1430, loss = 0.00504302\n",
      "Iteration 1431, loss = 0.00504467\n",
      "Iteration 1432, loss = 0.00502765\n",
      "Iteration 1433, loss = 0.00501810\n",
      "Iteration 1434, loss = 0.00498862\n",
      "Iteration 1435, loss = 0.00499416\n",
      "Iteration 1436, loss = 0.00498119\n",
      "Iteration 1437, loss = 0.00497124\n",
      "Iteration 1438, loss = 0.00496049\n",
      "Iteration 1439, loss = 0.00494935\n",
      "Iteration 1440, loss = 0.00494235\n",
      "Iteration 1441, loss = 0.00493989\n",
      "Iteration 1442, loss = 0.00493265\n",
      "Iteration 1443, loss = 0.00494165\n",
      "Iteration 1444, loss = 0.00490090\n",
      "Iteration 1445, loss = 0.00490273\n",
      "Iteration 1446, loss = 0.00489825\n",
      "Iteration 1447, loss = 0.00489510\n",
      "Iteration 1448, loss = 0.00488364\n",
      "Iteration 1449, loss = 0.00487387\n",
      "Iteration 1450, loss = 0.00487017\n",
      "Iteration 1451, loss = 0.00483850\n",
      "Iteration 1452, loss = 0.00484051\n",
      "Iteration 1453, loss = 0.00488297\n",
      "Iteration 1454, loss = 0.00487458\n",
      "Iteration 1455, loss = 0.00485488\n",
      "Iteration 1456, loss = 0.00482363\n",
      "Iteration 1457, loss = 0.00482352\n",
      "Iteration 1458, loss = 0.00479607\n",
      "Iteration 1459, loss = 0.00478431\n",
      "Iteration 1460, loss = 0.00478355\n",
      "Iteration 1461, loss = 0.00476631\n",
      "Iteration 1462, loss = 0.00476256\n",
      "Iteration 1463, loss = 0.00476338\n",
      "Iteration 1464, loss = 0.00475854\n",
      "Iteration 1465, loss = 0.00477824\n",
      "Iteration 1466, loss = 0.00473171\n",
      "Iteration 1467, loss = 0.00474172\n",
      "Iteration 1468, loss = 0.00473872\n",
      "Iteration 1469, loss = 0.00471442\n",
      "Iteration 1470, loss = 0.00469669\n",
      "Iteration 1471, loss = 0.00469458\n",
      "Iteration 1472, loss = 0.00468891\n",
      "Iteration 1473, loss = 0.00467114\n",
      "Iteration 1474, loss = 0.00469409\n",
      "Iteration 1475, loss = 0.00471231\n",
      "Iteration 1476, loss = 0.00465876\n",
      "Iteration 1477, loss = 0.00463461\n",
      "Iteration 1478, loss = 0.00463145\n",
      "Iteration 1479, loss = 0.00464297\n",
      "Iteration 1480, loss = 0.00463489\n",
      "Iteration 1481, loss = 0.00461586\n",
      "Iteration 1482, loss = 0.00461482\n",
      "Iteration 1483, loss = 0.00459466\n",
      "Iteration 1484, loss = 0.00461616\n",
      "Iteration 1485, loss = 0.00460185\n",
      "Iteration 1486, loss = 0.00456704\n",
      "Iteration 1487, loss = 0.00457038\n",
      "Iteration 1488, loss = 0.00455470\n",
      "Iteration 1489, loss = 0.00457072\n",
      "Iteration 1490, loss = 0.00455309\n",
      "Iteration 1491, loss = 0.00454422\n",
      "Iteration 1492, loss = 0.00454845\n",
      "Iteration 1493, loss = 0.00454431\n",
      "Iteration 1494, loss = 0.00451168\n",
      "Iteration 1495, loss = 0.00451088\n",
      "Iteration 1496, loss = 0.00449542\n",
      "Iteration 1497, loss = 0.00449546\n",
      "Iteration 1498, loss = 0.00449211\n",
      "Iteration 1499, loss = 0.00450409\n",
      "Iteration 1500, loss = 0.00448272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MatheusM\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede_neural_credit = MLPClassifier(max_iter = 1500, verbose = True, tol = 0.0000100, hidden_layer_sizes = (2,2),\n",
    "                                   solver = 'adam', activation = 'relu')\n",
    "\n",
    "rede_neural_credit.fit(X_credit_treino, y_credit_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.998\n"
     ]
    }
   ],
   "source": [
    "previsoes = rede_neural_credit.predict(X_credit_teste)\n",
    "\n",
    "print(accuracy_score(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_credit)\n",
    "cm.fit(X_credit_treino, y_credit_treino)\n",
    "cm.score(X_credit_teste,y_credit_teste)\n",
    "\n",
    "print(classification_report(y_credit_teste, previsoes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base de dados Census"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39514462\n",
      "Iteration 2, loss = 0.32634055\n",
      "Iteration 3, loss = 0.31396908\n",
      "Iteration 4, loss = 0.30622878\n",
      "Iteration 5, loss = 0.30209749\n",
      "Iteration 6, loss = 0.29777933\n",
      "Iteration 7, loss = 0.29441288\n",
      "Iteration 8, loss = 0.29034811\n",
      "Iteration 9, loss = 0.28788947\n",
      "Iteration 10, loss = 0.28477993\n",
      "Iteration 11, loss = 0.28220661\n",
      "Iteration 12, loss = 0.28001642\n",
      "Iteration 13, loss = 0.27704252\n",
      "Iteration 14, loss = 0.27423943\n",
      "Iteration 15, loss = 0.27198893\n",
      "Iteration 16, loss = 0.26978936\n",
      "Iteration 17, loss = 0.26754264\n",
      "Iteration 18, loss = 0.26589593\n",
      "Iteration 19, loss = 0.26422162\n",
      "Iteration 20, loss = 0.26103154\n",
      "Iteration 21, loss = 0.25792003\n",
      "Iteration 22, loss = 0.25589236\n",
      "Iteration 23, loss = 0.25384189\n",
      "Iteration 24, loss = 0.25173602\n",
      "Iteration 25, loss = 0.24947271\n",
      "Iteration 26, loss = 0.24623043\n",
      "Iteration 27, loss = 0.24508739\n",
      "Iteration 28, loss = 0.24371840\n",
      "Iteration 29, loss = 0.24186135\n",
      "Iteration 30, loss = 0.24016468\n",
      "Iteration 31, loss = 0.23742411\n",
      "Iteration 32, loss = 0.23640337\n",
      "Iteration 33, loss = 0.23404940\n",
      "Iteration 34, loss = 0.23257151\n",
      "Iteration 35, loss = 0.23079518\n",
      "Iteration 36, loss = 0.22731822\n",
      "Iteration 37, loss = 0.22723823\n",
      "Iteration 38, loss = 0.22715192\n",
      "Iteration 39, loss = 0.22381940\n",
      "Iteration 40, loss = 0.22125818\n",
      "Iteration 41, loss = 0.22008321\n",
      "Iteration 42, loss = 0.21838630\n",
      "Iteration 43, loss = 0.21700869\n",
      "Iteration 44, loss = 0.21720503\n",
      "Iteration 45, loss = 0.21381081\n",
      "Iteration 46, loss = 0.21218727\n",
      "Iteration 47, loss = 0.21053451\n",
      "Iteration 48, loss = 0.21014423\n",
      "Iteration 49, loss = 0.20924172\n",
      "Iteration 50, loss = 0.20695346\n",
      "Iteration 51, loss = 0.20633416\n",
      "Iteration 52, loss = 0.20387673\n",
      "Iteration 53, loss = 0.20287365\n",
      "Iteration 54, loss = 0.20240584\n",
      "Iteration 55, loss = 0.20078371\n",
      "Iteration 56, loss = 0.20001234\n",
      "Iteration 57, loss = 0.19763554\n",
      "Iteration 58, loss = 0.19679799\n",
      "Iteration 59, loss = 0.19447924\n",
      "Iteration 60, loss = 0.19520064\n",
      "Iteration 61, loss = 0.19397519\n",
      "Iteration 62, loss = 0.19152176\n",
      "Iteration 63, loss = 0.19022574\n",
      "Iteration 64, loss = 0.18905943\n",
      "Iteration 65, loss = 0.19109582\n",
      "Iteration 66, loss = 0.19105920\n",
      "Iteration 67, loss = 0.18822351\n",
      "Iteration 68, loss = 0.18522743\n",
      "Iteration 69, loss = 0.18460858\n",
      "Iteration 70, loss = 0.18343539\n",
      "Iteration 71, loss = 0.18414577\n",
      "Iteration 72, loss = 0.18384755\n",
      "Iteration 73, loss = 0.18148208\n",
      "Iteration 74, loss = 0.18046196\n",
      "Iteration 75, loss = 0.17889385\n",
      "Iteration 76, loss = 0.17853953\n",
      "Iteration 77, loss = 0.17644671\n",
      "Iteration 78, loss = 0.17902415\n",
      "Iteration 79, loss = 0.17886897\n",
      "Iteration 80, loss = 0.17648313\n",
      "Iteration 81, loss = 0.17444458\n",
      "Iteration 82, loss = 0.17374586\n",
      "Iteration 83, loss = 0.17193805\n",
      "Iteration 84, loss = 0.17221394\n",
      "Iteration 85, loss = 0.17266839\n",
      "Iteration 86, loss = 0.16953233\n",
      "Iteration 87, loss = 0.17120489\n",
      "Iteration 88, loss = 0.16945302\n",
      "Iteration 89, loss = 0.16923572\n",
      "Iteration 90, loss = 0.16814804\n",
      "Iteration 91, loss = 0.16821933\n",
      "Iteration 92, loss = 0.16752384\n",
      "Iteration 93, loss = 0.16483658\n",
      "Iteration 94, loss = 0.16640302\n",
      "Iteration 95, loss = 0.16370955\n",
      "Iteration 96, loss = 0.16362220\n",
      "Iteration 97, loss = 0.16372216\n",
      "Iteration 98, loss = 0.16445598\n",
      "Iteration 99, loss = 0.16249272\n",
      "Iteration 100, loss = 0.16469313\n",
      "Iteration 101, loss = 0.16120882\n",
      "Iteration 102, loss = 0.16043083\n",
      "Iteration 103, loss = 0.15909069\n",
      "Iteration 104, loss = 0.15847047\n",
      "Iteration 105, loss = 0.15779882\n",
      "Iteration 106, loss = 0.15872656\n",
      "Iteration 107, loss = 0.15893102\n",
      "Iteration 108, loss = 0.15917575\n",
      "Iteration 109, loss = 0.15597357\n",
      "Iteration 110, loss = 0.15728715\n",
      "Iteration 111, loss = 0.15389221\n",
      "Iteration 112, loss = 0.15375065\n",
      "Iteration 113, loss = 0.15227282\n",
      "Iteration 114, loss = 0.15502527\n",
      "Iteration 115, loss = 0.15378046\n",
      "Iteration 116, loss = 0.15209332\n",
      "Iteration 117, loss = 0.15390059\n",
      "Iteration 118, loss = 0.15137319\n",
      "Iteration 119, loss = 0.15267316\n",
      "Iteration 120, loss = 0.15053797\n",
      "Iteration 121, loss = 0.14861513\n",
      "Iteration 122, loss = 0.15040320\n",
      "Iteration 123, loss = 0.14789175\n",
      "Iteration 124, loss = 0.14676689\n",
      "Iteration 125, loss = 0.14747435\n",
      "Iteration 126, loss = 0.14915020\n",
      "Iteration 127, loss = 0.14921385\n",
      "Iteration 128, loss = 0.14711745\n",
      "Iteration 129, loss = 0.14576703\n",
      "Iteration 130, loss = 0.14677523\n",
      "Iteration 131, loss = 0.14508791\n",
      "Iteration 132, loss = 0.14775182\n",
      "Iteration 133, loss = 0.14569594\n",
      "Iteration 134, loss = 0.14338589\n",
      "Iteration 135, loss = 0.14573460\n",
      "Iteration 136, loss = 0.14425729\n",
      "Iteration 137, loss = 0.14279893\n",
      "Iteration 138, loss = 0.14096757\n",
      "Iteration 139, loss = 0.14376465\n",
      "Iteration 140, loss = 0.14047178\n",
      "Iteration 141, loss = 0.14036366\n",
      "Iteration 142, loss = 0.14065978\n",
      "Iteration 143, loss = 0.13922640\n",
      "Iteration 144, loss = 0.14143488\n",
      "Iteration 145, loss = 0.13886621\n",
      "Iteration 146, loss = 0.13651403\n",
      "Iteration 147, loss = 0.13807680\n",
      "Iteration 148, loss = 0.13835274\n",
      "Iteration 149, loss = 0.13838903\n",
      "Iteration 150, loss = 0.13900703\n",
      "Iteration 151, loss = 0.13878048\n",
      "Iteration 152, loss = 0.13971827\n",
      "Iteration 153, loss = 0.13768764\n",
      "Iteration 154, loss = 0.13471940\n",
      "Iteration 155, loss = 0.13613258\n",
      "Iteration 156, loss = 0.13465200\n",
      "Iteration 157, loss = 0.13453823\n",
      "Iteration 158, loss = 0.13473088\n",
      "Iteration 159, loss = 0.13826820\n",
      "Iteration 160, loss = 0.13714095\n",
      "Iteration 161, loss = 0.13247129\n",
      "Iteration 162, loss = 0.13388466\n",
      "Iteration 163, loss = 0.13213000\n",
      "Iteration 164, loss = 0.13191106\n",
      "Iteration 165, loss = 0.13228406\n",
      "Iteration 166, loss = 0.13307497\n",
      "Iteration 167, loss = 0.13373677\n",
      "Iteration 168, loss = 0.12998215\n",
      "Iteration 169, loss = 0.13215467\n",
      "Iteration 170, loss = 0.13077946\n",
      "Iteration 171, loss = 0.13049855\n",
      "Iteration 172, loss = 0.13131111\n",
      "Iteration 173, loss = 0.13088668\n",
      "Iteration 174, loss = 0.13133227\n",
      "Iteration 175, loss = 0.13125411\n",
      "Iteration 176, loss = 0.13145355\n",
      "Iteration 177, loss = 0.12805859\n",
      "Iteration 178, loss = 0.12989804\n",
      "Iteration 179, loss = 0.13001299\n",
      "Iteration 180, loss = 0.12732665\n",
      "Iteration 181, loss = 0.12595293\n",
      "Iteration 182, loss = 0.12806044\n",
      "Iteration 183, loss = 0.12657750\n",
      "Iteration 184, loss = 0.12681967\n",
      "Iteration 185, loss = 0.12633178\n",
      "Iteration 186, loss = 0.12596287\n",
      "Iteration 187, loss = 0.12704317\n",
      "Iteration 188, loss = 0.12597405\n",
      "Iteration 189, loss = 0.12679472\n",
      "Iteration 190, loss = 0.12720153\n",
      "Iteration 191, loss = 0.12682981\n",
      "Iteration 192, loss = 0.12631536\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n",
      "\n",
      "\n",
      "0.8165813715455476\n"
     ]
    }
   ],
   "source": [
    "rede_neural_census = MLPClassifier(hidden_layer_sizes=(55,55,55), solver = 'adam',\n",
    "                                   max_iter=1500, verbose = True, activation='relu', tol = 1e-5)\n",
    "\n",
    "rede_neural_census.fit(X_census_treino, y_census_treino)\n",
    "previsoes = rede_neural_census.predict(X_census_teste)\n",
    "print('\\n')\n",
    "print(accuracy_score(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.88      0.88      3693\n",
      "        >50K       0.63      0.61      0.62      1192\n",
      "\n",
      "    accuracy                           0.82      4885\n",
      "   macro avg       0.75      0.75      0.75      4885\n",
      "weighted avg       0.82      0.82      0.82      4885\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAFnCAYAAABO7YvUAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAaCklEQVR4nO3de3RU9b338c9MEgISBAJyM9xE6WnBEkgtNFZs1QOCCIoRA4hcNMQCJmhVfFAMyBFEBbRKuURLBBohBQoWsYBEfApCCSRc4qlEEQIihAAJDUnIhMx+/kjPuHLQ9gEhu3zn/VqLZWbPZOe7l2vPO789M+BxHMcRAAAwyev2AAAA4PIh9AAAGEboAQAwjNADAGAYoQcAwLBQtwe41Px+v0pLSxUWFiaPx+P2OAAAXFaO46iyslL169eX13v++t1c6EtLS5WXl+f2GAAA1KqOHTuqQYMG5203F/qwsDBJ0paHJ+vs8VMuTwMEj+QDmdVfFK9wdxAgyPiuult5eXmB/v1v5kL/P5frzx4/pfKjJ1yeBgge4eHh1V+EVbo7CBBs6tSRpO98uZo34wEAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYaFuDwD7bho7VD/51WDJcXRq/2H9KeE5+UpK1XdOilrd1Fker1dH/rpHa8dO0bmzFWrWuaMe3rpUp744FNjH8gce19WtW6jXqxMC20Lr1VXTH7TXgpiBOpr9qRuHBlxxVr2/Uw+NSdXf8+fJ5zunx55Zor9szZMk9bnjRr08+QGFhHhVXu7TUynLtGX75yotq1DCsFv11GN9XZ4eF6NWQr9v3z7Fx8erTZs2gW2zZ8/Wddddp927d2vKlCkqLy9Xs2bN9Morr6hZs2ZauXKl1q1bp/nz50uSHMfRtGnTtGXLFr311ltq1apVbYyO76llt06KfXKU5nUZoIq/n9F/vvK0bpuarNLCInlDQzSvywB5PB7du+QV/fz/JGpTym/UOrar9qav0ZrE52vs62TeAc3vek/g9v1/eF2frVxP5IH/T5/vP6YnU5bJ7ziSpDff+lCFJ0qUu+W/5Pc7uuWuacpYtV2D7+uhCVMydKq4VDs2TtaZ0rPq0nOSbunRUT1uut7lo8CFuujQO46jbdu26eDBgxo8ePA/fWxOTo769eunqVOn1tju8/mUlJSkWbNmKSYmRunp6Xr22WeVmppa43FVVVWaOHGi8vPzlZ6erkaNGl3s2KhlR7M/1Rs39Jb/3DmFhNdRg2ubq/jAV8r/v1kqPnhEchw5jqNjOX/TNZ2qn0CiYruq8XWt9chf/yBJ2vzSAn32xw019nvj0P5q1C5Ky+OfqPVjAq5EZWUVevDRBZo1dbCGJM6TJD0x5k49lnCHvF6vCk+cVvHpMkU2ri/HcbQ44xNlfZiikBCvGl59lT5a/YwaN7rK5aPAxbjg0J88eVIrVqzQypUr1bp1a40aNUqSFB8fr/Ly8hqP7datm1JSUpSTk6PDhw8rLi5OkjR69Gj16tVLe/fuVUREhGJiYiRJcXFxmjZtmoqKigL78Pl8Gj9+vCQpLS1NdevWvagDhXv8587pBwNuV/+3XtS5Cp82Pf8bnfoiP3B/wzat1GP8cP1p9CRJUmVpuXLT12jHvHfV9D+u0/BNi3U6/+vAyt0bFqbbpz+hFYN/LaeqypVjAq40iU+kKXHEL/TjTlE1toeFheqZKRl68+2N+kl0O93So6MKT5So5MxZffjxp3pk/O9UfLpMI4fcouTEXi5Nj+/jgkKflJSkvLw89e/fX2lpaWrRokXgvqVLl37n99WrV0/9+vXTkCFDtH//fg0bNkytWrXSsWPHauyjTp06ioyMVEFBgSSprKxMCQkJ2r59u1atWkXkr2D7Vm/UK6s3qtsj9+vBdW/rN9f/p+Q4atmtkx7445va/uYSff7+JknS2rFTAt934rMv9d8ZH+gH/W8LhP5Hcb1VtP+QDm/Z6cahAFec3769UaGhIRo1tKcOHio87/6XUgZp6sSBShi/UL96cpGmPRenqiq/9h88rsxVE1R4okS/6P+S2kY10T13xbhwBPg+Luhd9yEhIfJ4PPJ6vfJ4PDXui4+P14ABA2r8mTKl+gl78uTJGjJkiCSpQ4cO6tOnjzIzM+X3+7/z50jS9u3bFRMTo+TkZCUnJ+vMmTMXfIBwV+MObdT65m+eGHJ+t0IN27ZSvcYN1emBvhq24Xf68JmZ2jy9+r0YHq9Xt0x8VHUi6n+zE49HVZXnAjc7PdBXuxaurLVjAK50ae9uVlbOAUXfOkl9H5it8nKfom+dpC1//Vx5XxyTVL2yHzH458rek69rmjZQWFiIhg26WV6vV82bNVS/3l20NWu/y0eCi3FBK/rZs2cHLt0PHz5c7dq108iRI9W9e/fvXNFXVVVpwYIFGjZsmCIiIiRVv74fGhqqli1bqrDwm98uKysrVVRUpObNm2vv3r2KjY1VUlKSHMfRjh079PTTT2vOnDnn/ZKBf18NWl6j+96dpXnR96j8ZJFuHHq3jud+rna/7K4+v3lOi3s9rKM7cwOPd/x+dex/m86drdDWWQvVsE0r/fC+Xlp02/DAY9r2/EmNVT+Af277hymBrw8eKlTnnz+nXR9P1dRXV2vbjv1avSRZXq9Hv1++Vbfd8kPVqROqu3tHa9GyLXr1hXidOXNWGzZ9qud+3d/Fo8DFuuDP0Tdp0kSjR4/WBx98oAcffFBffPHFP318SEiIMjMzlZGRIUk6cuSI1q9fr969e6tLly4qLi5Wdna2JGnFihWKjo7W1VdfLan6Ur4keTwevfzyy/r00081d+7cCx0ZLjq0eaf+8uI8jdi0SIk5q9Q5/i4tu2esbp/+hOTxqP9b/6XEnFVKzFmlvm9Wv8t+5dAndX2fnnp0z3sa+kGq1o2fphOffSlJuqppY9WJuEolRwrcPCzAhAlJd6ltVFN16TlJXXo+r9CQEE2fVP1eqtTXRqrg+Gn96GcTFXPbZN17V4zi+t/k7sC4KB7H+cfnLC6j/Px8paSk6OTJk6qqqtK4cePUt2/15zH37NmjF154QeXl5WrUqJFmzJihqKio8z5eJ0nZ2dkaPny45syZo549e37rz6qoqFBubq423p2k8qMnLvehAfiHFGdf9Ren3nF3ECDIVNSPV25urjp37qzw8PDz7q+V0NcmQg+4g9AD7vhXoeevwAUAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwLdXuAy2Vhw1MqOFvo9hhA0Ej5ny8ih7s5BhB8Kir+6d2s6AFcEpGRkW6PAOBbmF3R79o0SeFhlW6PAQSNyOsfV2RkpE5uS3B7FCCoRA94T0uWLPnO+1nRAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhoW4PgOC06v2demhMqv6eP0+StOK9LE17bY0qKs6pbesmWvTb0WoSGaGysgo9Mn6hcvbky+93NCPlft1zV4zL0wNXlkWrcjV74Y7A7dMlFfqqoESfr0/QkzM+0mdfnpLf7+ihezprwujukqSsPUf1+LRMlZZXqsrv19OPdNeDAzq5dQj4HmplRb9v3z517dpVAwYMCPz58ssvJUm7d+/WwIED1adPHw0fPlzHjx+XJK1cuVKJiYmBfTiOoxdffFF9+/bV119/XRtj4zL5fP8xPZmyTH7HkSTtyDmgcc8s0Yq0ccrd8qI6dmihZ19cLkma/PIqRdQP19+2TdeGlU9pzFOL9dWRU26OD1xxHrqns3JWj1DO6hHavnyYWlxTX29MukMzF2bp2uYNtHfNKG1fPkzzluZoa84ROY6juKRVmpx0s3JWj9Da1Pv165c+0ucHOfeuRJdsRf/qq69q0KBBatOmzXn35eTkqF+/fpo6dWqN7T6fT0lJSZo1a5ZiYmKUnp6uZ599VqmpqTUeV1VVpYkTJyo/P1/p6elq1KjRpRobtaysrEIPPrpAs6YO1pDE6tX8kj98ooeH9lS7NtdIkiZPuEcnT52RJP3x/Wylz6/+ha9NVBP1+mUnZazerifG3OnOAQBXuBmpf1WzyKuUGB8tx3FUVVX9C/fRwlJV+KrUsEG4KnxVen7szbojtp0kKapFAzVtXE9fHSvRDe0iXZweF+OSreibNGmiMWPGaMSIEVq7dq18Pl/gvpycHO3fv19xcXGKi4vT+vXrJUl79+5VRESEYmKqL8XGxcVp69atKioqCnyvz+fTY489ppKSEqWlpRH5K1ziE2lKHPEL/bhTVGBb3v4CnTtXpQFDX1eXnpM09qnFahBRV5J0+MhJtb72myeWqFaR+urrovP2C+BfO3GqTLMWZmn2xNskSR6PR6GhXg17co1u7Pc7/eKnrfWD9pGqGx6qh+//ceD7FizbpTNlPvWIbuXW6PgeLlnoR44cqTVr1ig5OVmbN29Wnz599Pvf/16SVK9ePfXr10/Lly/XjBkzNHnyZOXm5urYsWNq0aJFYB916tRRZGSkCgoKJEllZWVKSEjQRx99pOTkZNWtW/dSjQsX/PbtjQoNDdGooT1rbK+srNKf1u3S/FnDlbNpilo0b6iE8QslSX6/c95+QkI8tTIvYM2CjN0acPsNat+6UY3ti1/tp8Jtj+nU6bN6Yc4nNe57acE2TX5ji96bd5/q1Q2rxWlxqVzy1+i9Xm+NP5I0efJkDRkyRJLUoUMH9enTR5mZmfL7/d+6j5CQEEnS9u3bFRMTo+TkZCUnJ+vMmTOXelzUorR3Nysr54Cib52kvg/MVnm5T9G3TpIk9b7tRrVo3kher1cjh9yirTv2S6q+XH+04HRgH0eOFimqFZcOgYuRsfYzjRjYOXB73V8O6OuCEklSRP06ir/rh8r57+qFVoXvnIY88Z6WrvmbPln6oLr8RzNXZsb3d8lCv2jRIvXv318zZ85UbGys1q5dq8GDB6uqqkpz586tEWnHcRQaGqqWLVuqsLAwsL2yslJFRUVq3ry5JCk2NlZJSUlKTExUVFSUnn76aTnO+Ss8XBm2f5ii3C0vatfHU7V22eOqV6+Odn08VUmj79D763cHXpdfuWaHburaXpI0oE9XLXhnkyTpqyOn9OfMverXK9qlIwCuXEWnz+qLQ8WK7XptYFvGB59pypxP5DiOKnzn9IcP9umXParfZzUoabX+fsanLUuHql1UQ7fGxiVwyd6Md/ToUb3++utq3759je0hISHKzMxUeHi4Ro0apSNHjmj9+vV655131KZNGxUXFys7O1vdunXTihUrFB0drauvvlpS9aV8qfp1pJdffln33nuv5s6dqzFjxlyqsfFv4O47u+qrr4t0693T5fc7atu6id5+fZQkacqEe/WrJ99Rp9iJqqry65XJD6hDe1YWwIX6Ir9ILa+pr7CwkMC2mc/8Ur9KWa8f371QHo804PYblPzQT7Rl51f600f71bFdY/18cHrg8S89eat639L+23aPf2MepxaWyPn5+UpJSdHJkydVVVWlcePGqW/fvpKkPXv26IUXXlB5ebkaNWqkGTNmKCoqSitXrtS6des0f/78wH6ys7M1fPhwzZkzRz179vzWn1VRUaHc3Fx1jtqn8LDKy31oAP4h8vrHJUkntyW4PAkQXH404D0tWbJEnTt3Vnh4+Hn310roaxOhB9xB6AF3/KvQ81fgAgBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhhF6AAAMI/QAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGEboAQAwjNADAGAYoQcAwDBCDwCAYYQeAADDCD0AAIYRegAADCP0AAAYRugBADCM0AMAYBihBwDAMEIPAIBhhB4AAMMIPQAAhoW6PcCl5jiOJMl31d1SnTouTwMEj+bNX5Ik/WjAey5PAgSXpk2bSvqmf/+bx/mue65QJSUlysvLc3sMAABqVceOHdWgQYPztpsLvd/vV2lpqcLCwuTxeNweBwCAy8pxHFVWVqp+/fryes9/Rd5c6AEAwDd4Mx4AAIYRegAADCP0AAAYRugBADCM0AMAYBihh2uOHTv2nfdt3ry5FicBggvnXnAh9HDNI488opKSkvO2z507V2PHjnVhIiA4cO4FF0IP1/zsZz9TYmKifD6fJKm0tFRjx45VRkaG0tLS3B0OMIxzL7jwF+bAVRMmTFBpaamSk5OVlJSk1q1ba8aMGWrcuLHbowGmce4FD0IPV/n9fo0bN04ff/yxkpOTNXr0aLdHAoIC517w4NI9XOX1evXaa6/ppptuClxGBHD5ce4FD1b0cM2jjz4a+Lq0tFRZWVm6+eabFRYWJkmaN2+eW6MBpnHuBRdz/x49rhy9e/eucXvgwIEuTQIEF8694MKKHq47ffq0CgoKFBISoubNmysiIsLtkYCgwLkXHFjRwzUnTpzQxIkTtW3bNkVGRspxHBUXFys6OlrTp09Xq1at3B4RMIlzL7iwoodrRo4cqV69eikuLi7w2uC5c+e0fPlyvf/++1q8eLHLEwI2ce4FF951D9ccP35cgwcPDjzRSFJoaKji4+NVXFzs3mCAcZx7wYXQwzV169bVrl27ztu+a9cu1a1bt/YHAoIE515w4dI9XLN7924lJyerQYMGatGihaTqlcbp06f1xhtv6MYbb3R5QsAmzr3gQujhqsrKSu3du1fHjh2T3+9Xy5Yt1aVLF4WG8j5R4HLi3AseXLqHaw4ePKiwsDB169ZN7dq108GDB5WVlaXDhw+7PRpgGudecCH0cM3jjz8uSdqwYYMSEhJUXFyswsJCPfTQQ/rzn//s8nSAXZx7wYVrNHBdamqqFi1apA4dOkiSRo0apcTERN15550uTwbYxrkXHFjRw3WO4wSeaCTp2muvlcfjcXEiIDhw7gUHQg/XHDx4UM8//7zCw8O1dOlSSVJZWZnS0tLUtGlTl6cD7OLcCy5cuodrli1bppycHPl8PuXl5UmSFi1apMzMTM2cOdPl6QC7OPeCCx+vw78Vv98vr5cLTUBt49yzi/+rcN3UqVMD/+WJBqhd7777rpYtW8a5ZxiX7uG67OxsSdLOnTtdngQILpWVlXr77bcVEhKiuLg4hYSEuD0SLgN+hQOAILVx40Z1795dP/3pT7Vhwwa3x8FlQugBIEhlZGRo0KBBuv/++wPvvoc9XLoHgCB0+PBhFRYWqkuXLpKkoqIiHTp0SG3atHF5MlxqrOjhuvDwcEnin8cEalFGRobuu+++wO24uDhW9Ubx8ToAAAxjRQ9XLVu2TNu2bQvczsrKUnp6uosTAYAthB6uatu2rRYuXBi4vXDhQrVr1869gQDAGEIPV/Xo0UOHDh1SQUGBjh8/rgMHDig2NtbtsQDADF6jh+tSU1NVVVUlr9crj8ejhIQEt0cCADMIPVx36tQpDR06VF6vV4sXL1ZkZKTbIwGAGXyOHq6LjIzUDTfcoNDQUCIPAJcYK3oAAAzjzXgAABhG6AEAMIzQAwBgGKEHAMAwQg8AgGGEHgAAwwg9AACGEXoAAAwj9AAAGPb/APIazp+xhievAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 576x396 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cm = ConfusionMatrix(rede_neural_census)\n",
    "cm.fit(X_census_treino, y_census_treino)\n",
    "cm.score(X_census_teste, y_census_teste)\n",
    "\n",
    "print(classification_report(y_census_teste, previsoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "145c14d8c45fa53258e73f0f31332f88a0083307ad3070aac1ab7d229133b30a"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
